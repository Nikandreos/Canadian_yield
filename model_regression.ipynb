{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of predictors are: 383\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Read the x and y data from CSV files\n",
    "#os.chdir(\"/Users/alrz-/Library/CloudStorage/OneDrive-Personal/Backup/Yield Curve Pricing/Data\")\n",
    "os.chdir(f\"{os.getcwd()}\\data\")\n",
    "\n",
    "raw_x = pd.read_csv(\"x.csv\", index_col=0)\n",
    "raw_initial_y = pd.read_csv(\"y.csv\", index_col=0)\n",
    "\n",
    "# Calculate the growth rate for each column in raw_y\n",
    "raw_y = raw_initial_y.pct_change()\n",
    "\n",
    "#replace Nan with 0 for raw_y\n",
    "raw_y = raw_y.fillna(0)\n",
    "raw_x.index=raw_y.index\n",
    "\n",
    "print(f\"the number of predictors are: {len(raw_x.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_list=[\n",
    " \"2-year yield\",\n",
    " \"3-year yield\",\n",
    " \"5-year yield\",\n",
    "\"7-year yield\",\n",
    "\"10-year yield\",\n",
    "\"long-term yield\",\n",
    "\"1-month yield\",\n",
    "\"2-month yield\",\n",
    "\"3-month yield\",\n",
    "\"6-month yield\",\n",
    "\"1-year yield\"\n",
    "]\n",
    "\n",
    "sort_list=[\n",
    "    \"1-month yield\",\n",
    "    \"2-month yield\",\n",
    "    \"3-month yield\",\n",
    "    \"6-month yield\",\n",
    "    \"1-year yield\",\n",
    "    \"2-year yield\",\n",
    "    \"3-year yield\",\n",
    "    \"5-year yield\",\n",
    "    \"7-year yield\",\n",
    "    \"10-year yield\",\n",
    "    \"long-term yield\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Create a folder to save the plots\n",
    "folder_path = \"results/plots correlation\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Plot autocorrelation lags for each column in raw_y\n",
    "num_lags = 5\n",
    "\n",
    "for col in raw_initial_y.columns:\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sm.graphics.tsa.plot_acf(raw_initial_y[col], lags=num_lags, ax=ax)\n",
    "    ax.set_xlabel('Lag')\n",
    "    ax.set_ylabel('Autocorrelation')\n",
    "    ax.set_xlim(0, num_lags)  # Limit the number of lags\n",
    "    ax.set_title(\"\")\n",
    "    \n",
    "    # Save the plot in the folder\n",
    "    plot_path = os.path.join(folder_path, f\"{col}_autocorrelation.jpeg\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close(fig)\n",
    "    #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class DataNormalizer:\n",
    "    def __init__(self):\n",
    "        self.scaler = MinMaxScaler()\n",
    "    \n",
    "    def normalize_dataframe(self, dataframe):\n",
    "        normalized_dataframe = pd.DataFrame(self.scaler.fit_transform(dataframe), columns=dataframe.columns, index=dataframe.index)\n",
    "        return normalized_dataframe\n",
    "    \n",
    "    def denormalize_dataframe(self, normalized_dataframe):\n",
    "        denormalized_dataframe = pd.DataFrame(self.scaler.inverse_transform(normalized_dataframe), columns=normalized_dataframe.columns, index=normalized_dataframe.index)\n",
    "        return denormalized_dataframe\n",
    "\n",
    "# Usage:\n",
    "Xnormalizer = DataNormalizer()\n",
    "Ynormalizer = DataNormalizer()\n",
    "\n",
    "# Normalize data\n",
    "x_normalized = Xnormalizer.normalize_dataframe(raw_x)\n",
    "y_normalized = Ynormalizer.normalize_dataframe(raw_y)\n",
    "\n",
    "# Denormalize data\n",
    "x_denormalized = Xnormalizer.denormalize_dataframe(x_normalized)\n",
    "y_denormalized = Ynormalizer.denormalize_dataframe(y_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cut data shapes:\n",
      "x cut: (240, 383)\n",
      "y cut: (240, 11)\n",
      "Training set shapes:\n",
      "x_train: (172, 383)\n",
      "y_train: (172, 11)\n",
      "\n",
      "Validation set shapes:\n",
      "x_val: (19, 383)\n",
      "y_val: (19, 11)\n",
      "\n",
      "Test set shapes:\n",
      "x_test: (49, 383)\n",
      "y_test: (49, 11)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the resulting sets\n",
    "print(\"Cut data shapes:\")\n",
    "print(\"x cut:\", raw_x.shape)\n",
    "print(\"y cut:\", raw_y.shape)\n",
    "\n",
    "# Set the percentages for training, validation, and test sets\n",
    "train_percentage = 0.72\n",
    "val_percentage = 0.08\n",
    "test_percentage = 0.2\n",
    "\n",
    "# Calculate the sizes of each set\n",
    "total_samples = len(raw_x)\n",
    "train_size = int(total_samples * train_percentage)\n",
    "val_size = int(total_samples * val_percentage)\n",
    "test_size = total_samples - train_size - val_size\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "x_train = raw_x[:train_size]\n",
    "x_val = raw_x[train_size:train_size+val_size]\n",
    "x_test = raw_x[train_size+val_size:]\n",
    "\n",
    "y_train = raw_y[:train_size]\n",
    "y_val = raw_y[train_size:train_size+val_size]\n",
    "y_test = raw_y[train_size+val_size:]\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(\"Training set shapes:\")\n",
    "print(\"x_train:\", x_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"\\nValidation set shapes:\")\n",
    "print(\"x_val:\", x_val.shape)\n",
    "print(\"y_val:\", y_val.shape)\n",
    "print(\"\\nTest set shapes:\")\n",
    "print(\"x_test:\", x_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_mean = raw_initial_y.mean()\n",
    "all_std = raw_initial_y.std()\n",
    "\n",
    "train_mean = raw_initial_y[:train_size].mean()\n",
    "train_std = raw_initial_y[:train_size].std()\n",
    "\n",
    "val_mean = raw_initial_y[train_size:train_size+val_size].mean()\n",
    "val_std = raw_initial_y[train_size:train_size+val_size].std()\n",
    "\n",
    "test_mean = raw_initial_y[train_size+val_size:].mean()\n",
    "test_std = raw_initial_y[train_size+val_size:].std()\n",
    "\n",
    "data = {\n",
    "    'all_mean': all_mean,\n",
    "    'all_std': all_std,\n",
    "    'train_mean': train_mean,\n",
    "    'train_std': train_std,\n",
    "    'val_mean': val_mean,\n",
    "    'val_std': val_std,\n",
    "    'test_mean': test_mean,\n",
    "    'test_std': test_std\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = df.round(decimals=2)\n",
    "df.index=dep_list\n",
    "\n",
    "df = df.reindex(sort_list)\n",
    "\n",
    "df.to_csv(\"results/summary_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2, axis=0)\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred), axis=0)\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true), axis=0) * 100\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor col in columns:\\n    plt.figure(figsize=(2, 1.5))  # Adjust the figsize parameter to make the plots smaller\\n    plt.plot(y_train.index, y_train[col], label=f'y_train - {col}')\\n    plt.plot(y_val.index, y_val[col], label=f'y_val - {col}')\\n    plt.plot(y_test.index, y_test[col], label=f'y_test - {col}')\\n    plt.xlabel('Index')\\n    plt.ylabel('Value')\\n    plt.title(f'Plot of {col} in y_train, y_val, and y_test')\\n    plt.xticks(x_ticks,rotation=45) \\n    plt.legend()\\n    plt.show()\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Plotting the data\n",
    "num_x_values = 7  # Adjust the number of x values as needed\n",
    "x_ticks = raw_y.index[::len(raw_y.index) // num_x_values]\n",
    "'''\n",
    "for col in columns:\n",
    "    plt.figure(figsize=(2, 1.5))  # Adjust the figsize parameter to make the plots smaller\n",
    "    plt.plot(y_train.index, y_train[col], label=f'y_train - {col}')\n",
    "    plt.plot(y_val.index, y_val[col], label=f'y_val - {col}')\n",
    "    plt.plot(y_test.index, y_test[col], label=f'y_test - {col}')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'Plot of {col} in y_train, y_val, and y_test')\n",
    "    plt.xticks(x_ticks,rotation=45) \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_value_39051</th>\n",
       "      <th>lead_value_39052</th>\n",
       "      <th>lead_value_39053</th>\n",
       "      <th>lead_value_39054</th>\n",
       "      <th>lead_value_39055</th>\n",
       "      <th>lead_value_39056</th>\n",
       "      <th>lead_value_39063</th>\n",
       "      <th>lead_value_39064</th>\n",
       "      <th>lead_value_39065</th>\n",
       "      <th>lead_value_39066</th>\n",
       "      <th>lead_value_39067</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-08</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-09</th>\n",
       "      <td>0.049669</td>\n",
       "      <td>-0.017391</td>\n",
       "      <td>-0.015152</td>\n",
       "      <td>-0.016355</td>\n",
       "      <td>-0.015317</td>\n",
       "      <td>-0.015779</td>\n",
       "      <td>0.091743</td>\n",
       "      <td>0.047826</td>\n",
       "      <td>0.094017</td>\n",
       "      <td>0.059761</td>\n",
       "      <td>0.025180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-10</th>\n",
       "      <td>0.015773</td>\n",
       "      <td>0.011799</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.004751</td>\n",
       "      <td>-0.006667</td>\n",
       "      <td>-0.014028</td>\n",
       "      <td>0.033613</td>\n",
       "      <td>0.045643</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.045113</td>\n",
       "      <td>0.045614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-11</th>\n",
       "      <td>-0.083851</td>\n",
       "      <td>-0.081633</td>\n",
       "      <td>-0.061538</td>\n",
       "      <td>-0.047733</td>\n",
       "      <td>-0.033557</td>\n",
       "      <td>-0.010163</td>\n",
       "      <td>-0.085366</td>\n",
       "      <td>-0.059524</td>\n",
       "      <td>-0.079245</td>\n",
       "      <td>-0.086331</td>\n",
       "      <td>-0.100671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12</th>\n",
       "      <td>-0.020339</td>\n",
       "      <td>-0.019048</td>\n",
       "      <td>-0.030055</td>\n",
       "      <td>-0.030075</td>\n",
       "      <td>-0.027778</td>\n",
       "      <td>-0.028747</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.008439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003937</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07</th>\n",
       "      <td>0.093264</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>0.084158</td>\n",
       "      <td>0.082524</td>\n",
       "      <td>0.071090</td>\n",
       "      <td>0.046083</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.037594</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.077419</td>\n",
       "      <td>0.089888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08</th>\n",
       "      <td>0.018957</td>\n",
       "      <td>0.018779</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.035874</td>\n",
       "      <td>0.053097</td>\n",
       "      <td>0.057269</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.017964</td>\n",
       "      <td>0.020619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09</th>\n",
       "      <td>0.065116</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>0.046218</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.126866</td>\n",
       "      <td>0.123288</td>\n",
       "      <td>0.106667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.060606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10</th>\n",
       "      <td>-0.030568</td>\n",
       "      <td>-0.030303</td>\n",
       "      <td>-0.045833</td>\n",
       "      <td>-0.049383</td>\n",
       "      <td>-0.052209</td>\n",
       "      <td>-0.039683</td>\n",
       "      <td>-0.013245</td>\n",
       "      <td>-0.018293</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>0.021390</td>\n",
       "      <td>0.009524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11</th>\n",
       "      <td>-0.148649</td>\n",
       "      <td>-0.151786</td>\n",
       "      <td>-0.170306</td>\n",
       "      <td>-0.168831</td>\n",
       "      <td>-0.165254</td>\n",
       "      <td>-0.119835</td>\n",
       "      <td>0.087248</td>\n",
       "      <td>0.012422</td>\n",
       "      <td>-0.035294</td>\n",
       "      <td>-0.068063</td>\n",
       "      <td>-0.108491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>172 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         lead_value_39051  lead_value_39052  lead_value_39053  \\\n",
       "2004-08          0.000000          0.000000          0.000000   \n",
       "2004-09          0.049669         -0.017391         -0.015152   \n",
       "2004-10          0.015773          0.011799          0.000000   \n",
       "2004-11         -0.083851         -0.081633         -0.061538   \n",
       "2004-12         -0.020339         -0.019048         -0.030055   \n",
       "...                   ...               ...               ...   \n",
       "2018-07          0.093264          0.092308          0.084158   \n",
       "2018-08          0.018957          0.018779          0.041096   \n",
       "2018-09          0.065116          0.064516          0.052632   \n",
       "2018-10         -0.030568         -0.030303         -0.045833   \n",
       "2018-11         -0.148649         -0.151786         -0.170306   \n",
       "\n",
       "         lead_value_39054  lead_value_39055  lead_value_39056  \\\n",
       "2004-08          0.000000          0.000000          0.000000   \n",
       "2004-09         -0.016355         -0.015317         -0.015779   \n",
       "2004-10         -0.004751         -0.006667         -0.014028   \n",
       "2004-11         -0.047733         -0.033557         -0.010163   \n",
       "2004-12         -0.030075         -0.027778         -0.028747   \n",
       "...                   ...               ...               ...   \n",
       "2018-07          0.082524          0.071090          0.046083   \n",
       "2018-08          0.035874          0.053097          0.057269   \n",
       "2018-09          0.051948          0.046218          0.050000   \n",
       "2018-10         -0.049383         -0.052209         -0.039683   \n",
       "2018-11         -0.168831         -0.165254         -0.119835   \n",
       "\n",
       "         lead_value_39063  lead_value_39064  lead_value_39065  \\\n",
       "2004-08          0.000000          0.000000          0.000000   \n",
       "2004-09          0.091743          0.047826          0.094017   \n",
       "2004-10          0.033613          0.045643          0.035156   \n",
       "2004-11         -0.085366         -0.059524         -0.079245   \n",
       "2004-12          0.040000          0.008439          0.000000   \n",
       "...                   ...               ...               ...   \n",
       "2018-07          0.023256          0.037594          0.072464   \n",
       "2018-08          0.015152          0.057971          0.013514   \n",
       "2018-09          0.126866          0.123288          0.106667   \n",
       "2018-10         -0.013245         -0.018293          0.024096   \n",
       "2018-11          0.087248          0.012422         -0.035294   \n",
       "\n",
       "         lead_value_39066  lead_value_39067  \n",
       "2004-08          0.000000          0.000000  \n",
       "2004-09          0.059761          0.025180  \n",
       "2004-10          0.045113          0.045614  \n",
       "2004-11         -0.086331         -0.100671  \n",
       "2004-12         -0.003937          0.000000  \n",
       "...                   ...               ...  \n",
       "2018-07          0.077419          0.089888  \n",
       "2018-08          0.017964          0.020619  \n",
       "2018-09          0.100000          0.060606  \n",
       "2018-10          0.021390          0.009524  \n",
       "2018-11         -0.068063         -0.108491  \n",
       "\n",
       "[172 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.hist(y_normalized['lead_value_39051'])\n",
    "plt.xlabel('change%')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of y')\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Walk Forecast:\n",
      "MSE: 3.218682444856316\n",
      "MAE: 0.4284272807580294\n",
      "MAPE: inf\n",
      "RMSE: 1.7940686845425724\n",
      "\n",
      "\u001b[91mRandom Walk Forecast RMSE test:\n",
      "\u001b[0mlead_value_39051    1.794069\n",
      "lead_value_39052    1.556114\n",
      "lead_value_39053    1.027035\n",
      "lead_value_39054    0.873672\n",
      "lead_value_39055    0.627272\n",
      "lead_value_39056    0.285345\n",
      "lead_value_39063    0.777029\n",
      "lead_value_39064    2.017834\n",
      "lead_value_39065    2.003226\n",
      "lead_value_39066    2.272945\n",
      "lead_value_39067    2.080034\n",
      "dtype: float64\n",
      "\u001b[91mRandom Walk Forecast RMSE validation:\n",
      "\u001b[0mlead_value_39051    0.176516\n",
      "lead_value_39052    0.179680\n",
      "lead_value_39053    0.173248\n",
      "lead_value_39054    0.178644\n",
      "lead_value_39055    0.173225\n",
      "lead_value_39056    0.117138\n",
      "lead_value_39063    0.286420\n",
      "lead_value_39064    0.297420\n",
      "lead_value_39065    0.278198\n",
      "lead_value_39066    0.217200\n",
      "lead_value_39067    0.240097\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def rw(y):\n",
    "    # Random walk forecast\n",
    "    y_rw = y.shift(1).fillna(method='bfill')\n",
    "    \n",
    "    return y_rw\n",
    "\n",
    "yf_rw=rw(y_test)\n",
    "yv_rw=rw(y_val)\n",
    "\n",
    "#print(y_test[-3:])\n",
    "#print(rw(y_test)[-2:])\n",
    "\n",
    "mse_val_rw = mse(y_val, yv_rw)\n",
    "mae_val_rw = mae(y_val, yv_rw)\n",
    "mape_val_rw = mape(y_val, yv_rw)\n",
    "rmse_val_rw = rmse(y_val, yv_rw)\n",
    "\n",
    "mse_test_rw = mse(y_test, yf_rw)\n",
    "mae_test_rw = mae(y_test, yf_rw)\n",
    "mape_test_rw = mape(y_test, yf_rw)\n",
    "rmse_test_rw = rmse(y_test, yf_rw)\n",
    "print(f\"Random Walk Forecast:\\nMSE: {mse_test_rw[0]}\\nMAE: {mae_test_rw[0]}\\nMAPE: {mape_test_rw[0]}\\nRMSE: {rmse_test_rw[0]}\\n\")\n",
    "\n",
    "print(\"\\033[91mRandom Walk Forecast RMSE test:\\n\\033[0m\" + str(rmse_test_rw))\n",
    "print(\"\\033[91mRandom Walk Forecast RMSE validation:\\n\\033[0m\" + str(rmse_val_rw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS RMSE: 17.58040498358444\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "\n",
    "def run_ols(train_x, train_y, test_x):\n",
    "    # Create an empty DataFrame to store the predictions\n",
    "    predictions_df = pd.DataFrame(index=test_x.index)\n",
    "    \n",
    "    # Iterate over each column of the dependent variable\n",
    "    for col in train_y.columns:\n",
    "        # Extract the current column of the dependent variable\n",
    "        train_y_col = train_y[col]\n",
    "        #print(col,\"\\n\")\n",
    "        # Add a constant column to the train_x data\n",
    "        train_x_const = sm.add_constant(train_x)\n",
    "        # Fit the OLS model\n",
    "        #print(train_x_const.index,train_y_col.index)\n",
    "        model = sm.OLS(train_y_col, train_x_const)\n",
    "        results = model.fit()\n",
    "        \n",
    "        # Add a constant column to the test_x data\n",
    "        test_x_const = sm.add_constant(test_x,has_constant='add')\n",
    "        \n",
    "        # Make predictions using the test_x data\n",
    "        predictions = results.predict(test_x_const)\n",
    "        \n",
    "        # Store the predictions in the DataFrame\n",
    "        predictions_df[col] = predictions\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "# Run the OLS model\n",
    "\n",
    "ols_train_size=int(len(raw_x) * 0.8)\n",
    "\n",
    "fv_ols = run_ols(Xnormalizer.normalize_dataframe(raw_x[:ols_train_size-1]), \n",
    "Ynormalizer.normalize_dataframe(raw_y[:ols_train_size-1]), \n",
    "Xnormalizer.normalize_dataframe(raw_x[ols_train_size-1:]))\n",
    "\n",
    "rmse_ols = rmse(y_test, Ynormalizer.denormalize_dataframe(fv_ols))\n",
    "\n",
    "print(f\"OLS RMSE: {rmse_ols.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2020-07    0.074074\n",
       "2020-08   -0.103448\n",
       "2020-09   -0.115385\n",
       "2020-10    0.173913\n",
       "2020-11   -0.074074\n",
       "2020-12    0.000000\n",
       "2021-01    0.000000\n",
       "2021-02    0.000000\n",
       "2021-03    0.000000\n",
       "2021-04    0.000000\n",
       "2021-05    0.000000\n",
       "2021-06    0.000000\n",
       "2021-07    0.000000\n",
       "2021-08    0.000000\n",
       "2021-09    0.000000\n",
       "2021-10    0.000000\n",
       "2021-11    0.000000\n",
       "2021-12    0.000000\n",
       "2022-01    0.000000\n",
       "2022-02    0.000000\n",
       "2022-03    8.880000\n",
       "2022-04    0.032389\n",
       "2022-05    0.243137\n",
       "2022-06   -0.018927\n",
       "2022-07    0.138264\n",
       "2022-08    0.081921\n",
       "2022-09    0.010444\n",
       "2022-10    0.002584\n",
       "2022-11   -0.043814\n",
       "2022-12   -0.024259\n",
       "2023-01    0.160221\n",
       "2023-02   -0.121429\n",
       "2023-03   -0.032520\n",
       "2023-04    0.204482\n",
       "2023-05    0.067442\n",
       "2023-06    0.008715\n",
       "2023-07    0.032397\n",
       "2023-08    0.031381\n",
       "2023-09   -0.058824\n",
       "2023-10   -0.051724\n",
       "2023-11   -0.093182\n",
       "2023-12   -0.027569\n",
       "2024-01    0.072165\n",
       "2024-02    0.000000\n",
       "2024-03    0.000000\n",
       "2024-04    0.028846\n",
       "2024-05   -0.072430\n",
       "2024-06   -0.093199\n",
       "2024-07    0.000000\n",
       "Name: lead_value_39051, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test['lead_value_39051']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA Forecast:\n",
      "MSE: lead_value_39051   NaN\n",
      "lead_value_39052   NaN\n",
      "lead_value_39053   NaN\n",
      "lead_value_39054   NaN\n",
      "lead_value_39055   NaN\n",
      "lead_value_39056   NaN\n",
      "lead_value_39063   NaN\n",
      "lead_value_39064   NaN\n",
      "lead_value_39065   NaN\n",
      "lead_value_39066   NaN\n",
      "lead_value_39067   NaN\n",
      "dtype: float64\n",
      "MAE: lead_value_39051   NaN\n",
      "lead_value_39052   NaN\n",
      "lead_value_39053   NaN\n",
      "lead_value_39054   NaN\n",
      "lead_value_39055   NaN\n",
      "lead_value_39056   NaN\n",
      "lead_value_39063   NaN\n",
      "lead_value_39064   NaN\n",
      "lead_value_39065   NaN\n",
      "lead_value_39066   NaN\n",
      "lead_value_39067   NaN\n",
      "dtype: float64\n",
      "MAPE: lead_value_39051   NaN\n",
      "lead_value_39052   NaN\n",
      "lead_value_39053   NaN\n",
      "lead_value_39054   NaN\n",
      "lead_value_39055   NaN\n",
      "lead_value_39056   NaN\n",
      "lead_value_39063   NaN\n",
      "lead_value_39064   NaN\n",
      "lead_value_39065   NaN\n",
      "lead_value_39066   NaN\n",
      "lead_value_39067   NaN\n",
      "dtype: float64\n",
      "RMSE: lead_value_39051   NaN\n",
      "lead_value_39052   NaN\n",
      "lead_value_39053   NaN\n",
      "lead_value_39054   NaN\n",
      "lead_value_39055   NaN\n",
      "lead_value_39056   NaN\n",
      "lead_value_39063   NaN\n",
      "lead_value_39064   NaN\n",
      "lead_value_39065   NaN\n",
      "lead_value_39066   NaN\n",
      "lead_value_39067   NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "\n",
    "def fit_arima_model(y, train_percentage, val_percentage, test_percentage, order):\n",
    "    # Calculate the sizes of each set\n",
    "    if not isinstance(y.index, pd.DatetimeIndex):\n",
    "        y.index = pd.to_datetime(y.index)\n",
    "\n",
    "    if y.index.freq is None:\n",
    "        y = y.asfreq('MS')  # Replace 'MS' with the correct frequency for your data\n",
    "\n",
    "    total_samples = len(y)\n",
    "    train_size = int(total_samples * train_percentage)\n",
    "    val_size = int(total_samples * val_percentage)\n",
    "    test_size = total_samples - train_size - val_size\n",
    "\n",
    "    # Split the data into training, validation, and test sets\n",
    "    train_y = y[:train_size]\n",
    "    val_y = y[train_size:train_size+val_size]\n",
    "    test_y = y[train_size+val_size:]\n",
    "\n",
    "    # Create empty DataFrames to store the fitted values for validation and test sets\n",
    "    val_fitted_values_df = pd.DataFrame(index=val_y.index)\n",
    "    test_fitted_values_df = pd.DataFrame(index=test_y.index)\n",
    "\n",
    "    # Iterate over each column of the dependent variable\n",
    "    for col in train_y.columns:\n",
    "        # Extract the current column of the dependent variable\n",
    "        train_y_col = train_y[col]\n",
    "\n",
    "        # Fit the ARIMA model on the train set\n",
    "        model = sm.tsa.ARIMA(train_y_col, order=order)\n",
    "        model_fit = model.fit()\n",
    "\n",
    "        # Get the fitted values for the validation set\n",
    "        val_fitted_values = model_fit.predict(start=len(train_y_col), end=len(train_y_col) + len(val_y) - 1)\n",
    "\n",
    "        # Get the fitted values for the test set\n",
    "        test_fitted_values = model_fit.predict(start=len(train_y_col) + len(val_y), end=len(train_y_col) + len(val_y) + len(test_y) - 1)\n",
    "\n",
    "        # Store the fitted values in the respective DataFrames\n",
    "        val_fitted_values_df[col] = val_fitted_values\n",
    "        test_fitted_values_df[col] = test_fitted_values\n",
    "    \n",
    "    val_fitted_values_df.index = val_fitted_values_df.index.to_period('M')\n",
    "    test_fitted_values_df.index = test_fitted_values_df.index.to_period('M')\n",
    "\n",
    "    return val_fitted_values_df, test_fitted_values_df\n",
    "\n",
    "\n",
    "\n",
    "# Define the ARIMA order\n",
    "order = (1, 0, 0)\n",
    "\n",
    "# Split y into train, validation, and test sets\n",
    "train_percentage = 0.72\n",
    "val_percentage = 0.08\n",
    "test_percentage = 0.2\n",
    "\n",
    "\n",
    "val_fitted_values, test_fitted_values = fit_arima_model(raw_y, train_percentage, val_percentage, test_percentage, order)\n",
    "#print(test_fitted_values)\n",
    "\n",
    "fv_arima=test_fitted_values \n",
    "\n",
    "mse_arima = mse(y_test, fv_arima)\n",
    "mae_arima = mae(y_test, fv_arima)\n",
    "mape_arima = mape(y_test, fv_arima)\n",
    "rmse_arima = rmse(y_test, fv_arima)\n",
    "print(f\"ARIMA Forecast:\\nMSE: {mse_arima}\\nMAE: {mae_arima}\\nMAPE: {mape_arima}\\nRMSE: {rmse_arima}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nalpha=0.01\\nmodels,fv_train_lasso=lasso_regression(Xnormalizer.normalize_dataframe(x_train),\\n                                       Ynormalizer.normalize_dataframe(y_train), alpha)\\nfilenames = {col: f'lasso_model_{col}.joblib' for col in models.keys()}\\nsave_models(models, filenames)\\nfv_lasso=Ynormalizer.denormalize_dataframe(load_and_predict(filenames, Xnormalizer.normalize_dataframe(x_test)))\\nfv_lasso[:5]\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "import joblib\n",
    "\n",
    "def lasso_regression(x_train, y_train, alpha):\n",
    "    models = {}  # Dictionary to store models for each dependent variable\n",
    "    fitted_values = {}  # Dictionary to store fitted values for each dependent variable\n",
    "\n",
    "    for col in y_train.columns:\n",
    "        model = Lasso(alpha=alpha)\n",
    "        model.fit(x_train, y_train[col])\n",
    "        models[col] = model\n",
    "        fitted_values[col] = model.predict(x_train)\n",
    "\n",
    "    return models, fitted_values\n",
    "\n",
    "def save_models(models, filenames):\n",
    "    for col, model in models.items():\n",
    "        joblib.dump(model, filenames[col])\n",
    "\n",
    "def load_and_predict(filenames, x_test):\n",
    "    predicted_values = {}\n",
    "    for col, filename in filenames.items():\n",
    "        loaded_model = joblib.load(filename)\n",
    "        predicted_values[col] = loaded_model.predict(x_test)\n",
    "    return pd.DataFrame(predicted_values,index=x_test.index)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "'''\n",
    "alpha=0.01\n",
    "models,fv_train_lasso=lasso_regression(Xnormalizer.normalize_dataframe(x_train),\n",
    "                                       Ynormalizer.normalize_dataframe(y_train), alpha)\n",
    "filenames = {col: f'lasso_model_{col}.joblib' for col in models.keys()}\n",
    "save_models(models, filenames)\n",
    "fv_lasso=Ynormalizer.denormalize_dataframe(load_and_predict(filenames, Xnormalizer.normalize_dataframe(x_test)))\n",
    "fv_lasso[:5]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nalpha=0.1\\nfilenames = {col: f'ridge_model_{col}.joblib' for col in models.keys()}\\nmodels,fv_train_ridge=ridge_regression(Xnormalizer.normalize_dataframe(x_train),\\n                                       Ynormalizer.normalize_dataframe(y_train), alpha)\\nsave_models(models, filenames)\\nfv_ridge=Ynormalizer.denormalize_dataframe(load_and_predict(filenames, Xnormalizer.normalize_dataframe(x_test)))\\nfv_ridge['lead_value_39051'][:5],fv_lasso['lead_value_39051'][:15]\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "import joblib\n",
    "\n",
    "def ridge_regression(x_train, y_train, alpha):\n",
    "    models = {}  # Dictionary to store models for each dependent variable\n",
    "    fitted_values = {}  # Dictionary to store fitted values for each dependent variable\n",
    "\n",
    "    for col in y_train.columns:\n",
    "        model = Ridge(alpha=alpha)\n",
    "        model.fit(x_train, y_train[col])\n",
    "        models[col] = model\n",
    "        fitted_values[col] = model.predict(x_train)\n",
    "\n",
    "    return models, fitted_values\n",
    "\n",
    "'''\n",
    "def save_models(models, filenames):\n",
    "    for col, model in models.items():\n",
    "        joblib.dump(model, filenames[col])\n",
    "\n",
    "def load_and_predict(filenames, x_test):\n",
    "    predicted_values = {}\n",
    "    for col, filename in filenames.items():\n",
    "        loaded_model = joblib.load(filename)\n",
    "        predicted_values[col] = loaded_model.predict(x_test)\n",
    "    return pd.DataFrame(predicted_values,index=x_test.index) '''\n",
    "\n",
    "# Example usage:\n",
    "'''\n",
    "alpha=0.1\n",
    "filenames = {col: f'ridge_model_{col}.joblib' for col in models.keys()}\n",
    "models,fv_train_ridge=ridge_regression(Xnormalizer.normalize_dataframe(x_train),\n",
    "                                       Ynormalizer.normalize_dataframe(y_train), alpha)\n",
    "save_models(models, filenames)\n",
    "fv_ridge=Ynormalizer.denormalize_dataframe(load_and_predict(filenames, Xnormalizer.normalize_dataframe(x_test)))\n",
    "fv_ridge['lead_value_39051'][:5],fv_lasso['lead_value_39051'][:15]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nalpha=0.1\\nmodels,fv_train_sgd=sgd_regression(Xnormalizer.normalize_dataframe(x_train),\\n                                       Ynormalizer.normalize_dataframe(y_train), alpha)\\nfilenames = {col: f'sgd_model_{col}.joblib' for col in models.keys()}\\nsave_models(models, filenames)\\nfv_sgd=Ynormalizer.denormalize_dataframe(load_and_predict(filenames, Xnormalizer.normalize_dataframe(x_val)))\\nrmse(fv_sgd['lead_value_39051'],y_val['lead_value_39051'])\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "def sgd_regression(x_train, y_train, penalty='l1',alpha=0.1):\n",
    "    models = {}  # Dictionary to store models for each dependent variable\n",
    "    fitted_values = {}  # Dictionary to store fitted values for each dependent variable\n",
    "\n",
    "    for col in y_train.columns:\n",
    "        model = SGDRegressor(penalty=penalty, alpha=alpha)\n",
    "        model.fit(x_train, y_train[col])\n",
    "        models[col] = model\n",
    "        fitted_values[col] = model.predict(x_train)\n",
    "\n",
    "    return models, fitted_values\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "'''\n",
    "alpha=0.1\n",
    "models,fv_train_sgd=sgd_regression(Xnormalizer.normalize_dataframe(x_train),\n",
    "                                       Ynormalizer.normalize_dataframe(y_train), alpha)\n",
    "filenames = {col: f'sgd_model_{col}.joblib' for col in models.keys()}\n",
    "save_models(models, filenames)\n",
    "fv_sgd=Ynormalizer.denormalize_dataframe(load_and_predict(filenames, Xnormalizer.normalize_dataframe(x_val)))\n",
    "rmse(fv_sgd['lead_value_39051'],y_val['lead_value_39051'])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nn_estimators = 100\\nmax_depth = 10\\nmodels, fv_train_random_forest = random_forest_regression(x_train, y_train, n_estimators, max_depth)\\nfilenames = {col: f'random_forest_model_{col}.joblib' for col in models.keys()}\\nsave_models(models, filenames)\\nfv_rf=load_and_predict(filenames, x_test)\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "def random_forest_regression(x_train, y_train, n_estimators, max_depth):\n",
    "    models = {}  # Dictionary to store models for each dependent variable\n",
    "    fitted_values = {}  # Dictionary to store fitted values for each dependent variable\n",
    "\n",
    "    for col in y_train.columns:\n",
    "        model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth,random_state=42)\n",
    "        model.fit(x_train, y_train[col])\n",
    "        models[col] = model\n",
    "        fitted_values[col] = model.predict(x_train)\n",
    "\n",
    "    return models, fitted_values\n",
    "\n",
    "# Example usage:\n",
    "'''\n",
    "n_estimators = 100\n",
    "max_depth = 10\n",
    "models, fv_train_random_forest = random_forest_regression(x_train, y_train, n_estimators, max_depth)\n",
    "filenames = {col: f'random_forest_model_{col}.joblib' for col in models.keys()}\n",
    "save_models(models, filenames)\n",
    "fv_rf=load_and_predict(filenames, x_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "def xgboost_regression(x_train, y_train, n_estimators, max_depth):\n",
    "    models = {}  # Dictionary to store models for each dependent variable\n",
    "    fitted_values = {}  # Dictionary to store fitted values for each dependent variable\n",
    "\n",
    "    for col in y_train.columns:\n",
    "        model = xgb.XGBRegressor(n_estimators=n_estimators, max_depth=max_depth,random_state=42)\n",
    "        model.fit(x_train, y_train[col])\n",
    "        models[col] = model\n",
    "        fitted_values[col] = model.predict(x_train)\n",
    "\n",
    "    return models, fitted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "def extra_tree_regression(x_train, y_train, n_estimators, max_depth, return_feature_importance=False):\n",
    "    models = {}  \n",
    "    fitted_values = {}  \n",
    "\n",
    "    for col in y_train.columns:\n",
    "        model = ExtraTreesRegressor(n_estimators=n_estimators, max_depth=max_depth,random_state=42)\n",
    "        model.fit(x_train, y_train[col])\n",
    "        models[col] = model\n",
    "        fitted_values[col] = model.predict(x_train)\n",
    "    \n",
    "    if return_feature_importance:\n",
    "        feature_importance = {col: model.feature_importances_ for col, model in models.items()}\n",
    "        return models, fitted_values, feature_importance\n",
    "    \n",
    "    return models, fitted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nn_components = 2\\nmodels, fv_train_pls = pls_regression(x_train, y_train, n_components)\\nfilenames = {col: f'pls_model_{col}.joblib' for col in models.keys()}\\nsave_models(models, filenames)\\nfv_pls = PLS_load_and_predict(filenames, x_test)\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "def PLS_save_models(models, filenames):\n",
    "    for col, model in models.items():\n",
    "        joblib.dump(model, filenames[col])\n",
    "\n",
    "def PLS_load_and_predict(filenames, x_test):\n",
    "    predicted_values = {}\n",
    "    for col, filename in filenames.items():\n",
    "        loaded_model = joblib.load(filename)\n",
    "        items= loaded_model.predict(x_test)\n",
    "        predicted_values[col] = [item[0] for item in items]\n",
    "    #print(f\"predicted values shape {predicted_values}\")\n",
    "    return pd.DataFrame(predicted_values,index=x_test.index)\n",
    "\n",
    "def pls_regression(x_train, y_train, n_components):\n",
    "    models = {}  # Dictionary to store models for each dependent variable\n",
    "    fitted_values = {}  # Dictionary to store fitted values for each dependent variable\n",
    "\n",
    "    for col in y_train.columns:\n",
    "        model = PLSRegression(n_components=n_components)\n",
    "        model.fit(x_train, y_train[col])\n",
    "        models[col] = model\n",
    "        temp = model.predict(x_train)\n",
    "        fitted_values[col] = [item[0] for item in temp]  # Remove brackets from items in temp\n",
    "        \n",
    "    return models, fitted_values\n",
    "\n",
    "# Example usage:\n",
    "'''\n",
    "n_components = 2\n",
    "models, fv_train_pls = pls_regression(x_train, y_train, n_components)\n",
    "filenames = {col: f'pls_model_{col}.joblib' for col in models.keys()}\n",
    "save_models(models, filenames)\n",
    "fv_pls = PLS_load_and_predict(filenames, x_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.svm import SVR\\ndef svm_regression(x_train, y_train, kernel='rbf', C=1.0, epsilon=0.1):\\n    models = {}  # Dictionary to store models for each dependent variable\\n    fitted_values = {}  # Dictionary to store fitted values for each dependent variable\\n\\n    for col in y_train.columns:\\n        model = SVR(kernel=kernel, C=C, epsilon=epsilon)\\n        model.fit(x_train, y_train[col])\\n        models[col] = model\\n        fitted_values[col] = model.predict(x_train)\\n\\n    return models, fitted_values\\n\\n# Example usage:\\nmodels, fv_train_svm = svm_regression(x_train, y_train, kernel='linear', C=0.5, epsilon=0.01)\\nfilenames = {col: f'svm_model_{col}.joblib' for col in models.keys()}\\nsave_models(models, filenames)\\nfv_svm = load_and_predict(filenames, x_test)\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.svm import SVR\n",
    "def svm_regression(x_train, y_train, kernel='rbf', C=1.0, epsilon=0.1):\n",
    "    models = {}  # Dictionary to store models for each dependent variable\n",
    "    fitted_values = {}  # Dictionary to store fitted values for each dependent variable\n",
    "\n",
    "    for col in y_train.columns:\n",
    "        model = SVR(kernel=kernel, C=C, epsilon=epsilon)\n",
    "        model.fit(x_train, y_train[col])\n",
    "        models[col] = model\n",
    "        fitted_values[col] = model.predict(x_train)\n",
    "\n",
    "    return models, fitted_values\n",
    "\n",
    "# Example usage:\n",
    "models, fv_train_svm = svm_regression(x_train, y_train, kernel='linear', C=0.5, epsilon=0.01)\n",
    "filenames = {col: f'svm_model_{col}.joblib' for col in models.keys()}\n",
    "save_models(models, filenames)\n",
    "fv_svm = load_and_predict(filenames, x_test)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Alireza\\\\OneDrive\\\\Backup\\\\Yield Curve Pricing\\\\data'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsaved_filenames = {col: f'model_{col}.keras' for col in y_train.columns}\\nmodels=train_neural_network(x_train, y_train, saved_filenames)\\npredicted_values_nn = load_and_predict_nn(saved_filenames, x_test)\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running the NN model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def train_neural_network(X_train, y_train, save_filenames, layer1=64, layer2=32, epochs=10):\n",
    "    models = {}  # Dictionary to store models for each dependent variable\n",
    "\n",
    "    # Iterate over each column of the dependent variable\n",
    "    for col in y_train.columns:\n",
    "        # Create a new model\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(X_train.shape[1],)))\n",
    "        model.add(Dense(layer1, activation='relu'))\n",
    "        model.add(Dense(layer2, activation='relu'))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "        # Fit the model on the training data\n",
    "        model.fit(X_train, y_train[col], epochs=epochs, batch_size=32, verbose=0)\n",
    "\n",
    "        # Save the model for future use\n",
    "        models[col] = model\n",
    "        model.save(f'nueral network/{save_filenames[col]}')\n",
    "\n",
    "    return models\n",
    "\n",
    "# Compute the MSE for x_val\n",
    "\n",
    "def load_and_predict_nn(filenames, x_test):\n",
    "    predicted_values = {}\n",
    "    for col, filename in filenames.items():\n",
    "        model = tf.keras.models.load_model(f'nueral network/{filename}')\n",
    "        predicted_values[col] = model.predict(x_test).flatten()\n",
    "    return pd.DataFrame(predicted_values, index=x_test.index)\n",
    "\n",
    "\n",
    "# Load the saved models\n",
    "\"\"\"\n",
    "saved_filenames = {col: f'model_{col}.keras' for col in y_train.columns}\n",
    "models=train_neural_network(x_train, y_train, saved_filenames)\n",
    "predicted_values_nn = load_and_predict_nn(saved_filenames, x_test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsaved_filenames = {col: f'model_lstm_{col}.keras' for col in y_train.columns}\\nmodels=train_neural_LSTM(x_train, y_train, saved_filenames)\\npredicted_values_nn = load_and_predict_nn_LSTM(saved_filenames, x_test)\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Reshape, LSTM\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def train_neural_LSTM(X_train, y_train, save_filenames, layer1=64, layer2=32, lstm_units=32, epochs=10):\n",
    "    models = {}  # Dictionary to store models for each dependent variable\n",
    "\n",
    "    # Iterate over each column of the dependent variable\n",
    "    for col in y_train.columns:\n",
    "        # Create a new model\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(X_train.shape[1],)))\n",
    "        model.add(Dense(layer1, activation='relu'))\n",
    "        model.add(Dense(layer2, activation='relu'))\n",
    "        model.add(tf.keras.layers.Reshape((layer2, 1)))\n",
    "        model.add(LSTM(lstm_units))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "        # Fit the model on the training data\n",
    "        model.fit(X_train, y_train[col], epochs=epochs, batch_size=32, verbose=0)\n",
    "\n",
    "        # Save the model for future use\n",
    "        models[col] = model\n",
    "        model.save(f'nueral network/{save_filenames[col]}')\n",
    "\n",
    "    return models\n",
    "\n",
    "def load_and_predict_nn_LSTM(filenames, x_test):\n",
    "    predicted_values = {}\n",
    "    for col, filename in filenames.items():\n",
    "        model = tf.keras.models.load_model(f'nueral network/{filename}')\n",
    "        predicted_values[col] = model.predict(x_test).flatten()\n",
    "    return pd.DataFrame(predicted_values, index=x_test.index)\n",
    "\n",
    "# Load the saved models\n",
    "'''\n",
    "saved_filenames = {col: f'model_lstm_{col}.keras' for col in y_train.columns}\n",
    "models=train_neural_LSTM(x_train, y_train, saved_filenames)\n",
    "predicted_values_nn = load_and_predict_nn_LSTM(saved_filenames, x_test)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", 'Do not pass an `input_shape`', module='tensorflow')\n",
    "\n",
    "def nn_pls_model(raw_x,raw_y,n_components=5,epochs=15,layer1=64,layer2=32):\n",
    "    predicted_values_test={}\n",
    "    predicted_values_val={}\n",
    "    for i in range(len(raw_y.columns)):\n",
    "\n",
    "        # Create the PLS regression model\n",
    "        pls_model = PLSRegression(n_components=n_components)\n",
    "\n",
    "        # Fit the model to the data\n",
    "        pls_model.fit(x_normalized, y_normalized.iloc[:, i])\n",
    "\n",
    "        # Get the top 20 variable outcomes\n",
    "        top_20_outcomes = pls_model.x_scores_\n",
    "        x_shrunk= pls_model.transform(x_normalized)\n",
    "        x_shrunk = pd.DataFrame(pls_model.transform(x_normalized.iloc[:int(0.72*len(x_normalized)),]) )\n",
    "        # Apply PLS shrinkage on x_test\n",
    "        x_test_shrunk = pd.DataFrame(pls_model.transform(x_normalized.iloc[int(0.8*len(x_normalized)):,]))\n",
    "\n",
    "        train_percentage = 0.72\n",
    "        val_percentage = 0.08\n",
    "        test_percentage = 0.2\n",
    "\n",
    "        # Calculate the sizes of each set\n",
    "        total_samples = len(x_normalized)\n",
    "        train_size = int(total_samples * train_percentage)\n",
    "        val_size = int(total_samples * val_percentage)\n",
    "        test_size = total_samples - train_size - val_size\n",
    "\n",
    "        # Split the data into training, validation, and test sets\n",
    "        x_train = top_20_outcomes[:train_size]\n",
    "        x_val = top_20_outcomes[train_size:train_size+val_size]\n",
    "        x_test = top_20_outcomes[train_size+val_size:]\n",
    "\n",
    "        y_train = raw_y[:train_size]\n",
    "        y_val = raw_y[train_size:train_size+val_size]\n",
    "        y_test = raw_y[train_size+val_size:]\n",
    "\n",
    "        x_train=pd.DataFrame(x_train)\n",
    "        y_train=pd.DataFrame(y_train)   \n",
    "        x_test=pd.DataFrame(x_test)\n",
    "        y_test=pd.DataFrame(y_test)\n",
    "\n",
    "        # Add layers to the model\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(x_train.shape[1],)))\n",
    "        model.add(Dense(layer1, activation='relu'))\n",
    "        model.add(Dense(layer2, activation='relu'))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        \n",
    "        # Fit the model to the training data\n",
    "        model.fit(x_train, y_train, epochs=epochs, batch_size=8,verbose=0)\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        y_val_pred = model.predict(x_val)\n",
    "        y_test_pred=model.predict(x_test)\n",
    "        predicted_values_val[raw_y.columns[i]] = y_val_pred.flatten()\n",
    "        predicted_values_test[raw_y.columns[i]] = y_test_pred.flatten()\n",
    "    return(pd.DataFrame(predicted_values_val,index=y_val.index),\n",
    "           pd.DataFrame(predicted_values_test,index=y_test.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 99ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002405F4388B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "WARNING:tensorflow:6 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002405F4388B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "nn_pls_val,nn_pls_test=nn_pls_model(raw_x,raw_y,n_components=2,epochs=1,layer1=4,layer2=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part onward fine tunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe models are:\\n1- OLS\\n2- ARIMA\\n3- LASSO\\n4- Ridge\\n5- SGD\\n6- Random Forest\\n7- XGboost\\n8- Extreme Trees\\n9-PLS\\n10- Nueral Networks\\n11-LSTM\\n12- PLS+NN\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The models are:\n",
    "1- OLS\n",
    "2- ARIMA\n",
    "3- LASSO\n",
    "4- Ridge\n",
    "5- SGD\n",
    "6- Random Forest\n",
    "7- XGboost\n",
    "8- Extreme Trees\n",
    "9-PLS\n",
    "10- Nueral Networks\n",
    "11-LSTM\n",
    "12- PLS+NN\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Walk Error: 15.314575727287238\n",
      "OLS Error: 17.580404983584437\n"
     ]
    }
   ],
   "source": [
    "# Random walk Error\n",
    "print(f\"Random Walk Error: {sum(rmse_test_rw)}\")\n",
    "# OLS Error\n",
    "print(f\"OLS Error: {sum(rmse_ols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alireza\\AppData\\Local\\Temp\\ipykernel_936\\2827984830.py:3: UserWarning: This is a warning message\n",
      "  warnings.warn(\"This is a warning message\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA Forecast for (1, 0, 0): RMSE: nan\n",
      "ARIMA Forecast for (0, 1, 0): RMSE: nan\n",
      "ARIMA Forecast for (0, 0, 1): RMSE: nan\n",
      "ARIMA Forecast for (1, 1, 0): RMSE: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA Forecast for (0, 1, 1): RMSE: nan\n",
      "ARIMA Forecast for (2, 0, 0): RMSE: nan\n",
      "ARIMA Forecast for (0, 2, 0): RMSE: nan\n",
      "ARIMA Forecast for (0, 0, 2): RMSE: nan\n",
      "ARIMA Forecast for (2, 1, 0): RMSE: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA Forecast for (0, 2, 1): RMSE: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.warn(\"This is a warning message\", UserWarning)\n",
    "\n",
    "arima_settings = [(1, 0, 0), (0, 1, 0), (0, 0, 1), (1, 1, 0), (0, 1, 1), (2, 0, 0), (0, 2, 0), (0, 0, 2), (2, 1, 0), (0, 2, 1)]  # Updated ARIMA settings\n",
    "\n",
    "rmse_val_arima = {}\n",
    "for arima_setting in arima_settings:\n",
    "    order = arima_setting\n",
    "    val_fitted_values, test_fitted_values = fit_arima_model(raw_y, train_percentage, val_percentage, test_percentage, order)\n",
    "    rmse_arima = rmse(y_val, val_fitted_values)\n",
    "    errors = sum(rmse_arima)\n",
    "    print(f\"ARIMA Forecast for {arima_setting}: RMSE: {errors}\")\n",
    "    rmse_val_arima[f\"ARIMA Forecast for {arima_setting}\"] = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.001, Error: 2.589983331988494\n",
      "Alpha: 0.01, Error: 2.037502052703111\n",
      "Alpha: 0.1, Error: 2.0374646555559357\n",
      "Alpha: 1.0, Error: 2.0374646555559357\n"
     ]
    }
   ],
   "source": [
    "alpha_values = [0.001,0.01, 0.1, 1.0]  # List of alpha values to iterate over\n",
    "rmse_val_lasso = {}\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    models, fv_train_lasso = lasso_regression(Xnormalizer.normalize_dataframe(x_train),\n",
    "                                              Ynormalizer.normalize_dataframe(y_train), alpha)\n",
    "    filenames = {col: f'lasso_model_{col}.joblib' for col in models.keys()}\n",
    "    save_models(models, filenames)\n",
    "    fv_lasso = Ynormalizer.denormalize_dataframe(load_and_predict(filenames, Xnormalizer.normalize_dataframe(x_val)))\n",
    "    l = rmse(y_val, fv_lasso)\n",
    "    errors = sum(l)\n",
    "    print(f\"Alpha: {alpha}, Error: {errors}\")\n",
    "    rmse_val_lasso[f\"Alpha: {alpha}\"] = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.001, Error: 23.05976716607876\n",
      "Alpha: 0.01, Error: 17.879041617087108\n",
      "Alpha: 0.1, Error: 9.048239507224665\n",
      "Alpha: 1.0, Error: 3.338672615059912\n"
     ]
    }
   ],
   "source": [
    "alpha_values = [0.001, 0.01, 0.1, 1.0]  # List of alpha values to iterate over\n",
    "\n",
    "rmse_val_ridge = {}\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    models, fv_train_ridge = ridge_regression(Xnormalizer.normalize_dataframe(x_train),\n",
    "                                              Ynormalizer.normalize_dataframe(y_train), alpha)\n",
    "    filenames = {col: f'ridge_model_{col}.joblib' for col in models.keys()}\n",
    "    save_models(models, filenames)\n",
    "    fv_ridge = Ynormalizer.denormalize_dataframe(load_and_predict(filenames, Xnormalizer.normalize_dataframe(x_val)))\n",
    "    l = rmse(y_val, fv_ridge)\n",
    "    errors = sum(l)\n",
    "    print(f\"Alpha: {alpha}, Error: {errors}\")\n",
    "    rmse_val_ridge[f\"Alpha: {alpha}\"] = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penalty: l1, Alpha: 0.001, Error: 2.0746770550221525\n",
      "Penalty: l1, Alpha: 0.01, Error: 2.128132084804599\n",
      "Penalty: l1, Alpha: 0.1, Error: 1.9574430484500356\n",
      "Penalty: l1, Alpha: 1.0, Error: 1.9940130369567597\n",
      "Penalty: l2, Alpha: 0.001, Error: 2.2043706353908856\n",
      "Penalty: l2, Alpha: 0.01, Error: 2.1564112797515356\n",
      "Penalty: l2, Alpha: 0.1, Error: 2.247671698065401\n",
      "Penalty: l2, Alpha: 1.0, Error: 2.185811975297578\n",
      "Penalty: elasticnet, Alpha: 0.001, Error: 2.129909089421397\n",
      "Penalty: elasticnet, Alpha: 0.01, Error: 2.043246314721519\n",
      "Penalty: elasticnet, Alpha: 0.1, Error: 2.398198287859678\n",
      "Penalty: elasticnet, Alpha: 1.0, Error: 1.9875520490825713\n"
     ]
    }
   ],
   "source": [
    "penalties = ['l1', 'l2','elasticnet']  # List of penalty values to iterate over\n",
    "alphas = [0.001, 0.01, 0.1, 1.0]  # List of alpha values to iterate over\n",
    "rmse_val_sgd = {}\n",
    "for penalty in penalties:\n",
    "    for alpha in alphas:\n",
    "        models, fv_train_sgd = sgd_regression(Xnormalizer.normalize_dataframe(x_train),\n",
    "                                              Ynormalizer.normalize_dataframe(y_train), penalty=penalty, alpha=alpha)\n",
    "        filenames = {col: f'sgd_model_{col}.joblib' for col in models.keys()}\n",
    "        save_models(models, filenames)\n",
    "        fv_sgd = Ynormalizer.denormalize_dataframe(load_and_predict(filenames, Xnormalizer.normalize_dataframe(x_val)))\n",
    "        l = rmse(fv_sgd, y_val)\n",
    "        error = sum(l)\n",
    "        print(f\"Penalty: {penalty}, Alpha: {alpha}, Error: {error}\")\n",
    "        rmse_val_sgd[f\"Penalty: {penalty}, Alpha: {alpha}\"] = error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (n_estimators=50, max_depth=5): 2.277534933868784\n",
      "Random Forest (n_estimators=50, max_depth=10): 2.3029129058014357\n",
      "Random Forest (n_estimators=50, max_depth=15): 2.271085751629758\n",
      "Random Forest (n_estimators=100, max_depth=5): 2.2906820303536093\n",
      "Random Forest (n_estimators=100, max_depth=10): 2.3009040501067832\n",
      "Random Forest (n_estimators=100, max_depth=15): 2.2886276470843643\n",
      "Random Forest (n_estimators=150, max_depth=5): 2.288167971727024\n",
      "Random Forest (n_estimators=150, max_depth=10): 2.2998969474485342\n",
      "Random Forest (n_estimators=150, max_depth=15): 2.2891010141240935\n"
     ]
    }
   ],
   "source": [
    "n_estimators_values = [50, 100, 150]\n",
    "max_depth_values = [5, 10, 15]\n",
    "rmse_val_rf = {}\n",
    "\n",
    "for n_estimators in n_estimators_values:\n",
    "    for max_depth in max_depth_values:\n",
    "        models, fv_train_random_forest = random_forest_regression(Xnormalizer.normalize_dataframe(x_train), \n",
    "                                                                  Ynormalizer.normalize_dataframe(y_train)\n",
    "                                                                  , n_estimators, max_depth)\n",
    "        filenames = {col: f'random_forest_model_{col}.joblib' for col in models.keys()}\n",
    "        save_models(models, filenames)\n",
    "        fv_rf = Ynormalizer.denormalize_dataframe(load_and_predict(filenames, Xnormalizer.normalize_dataframe(x_val)))\n",
    "        l = rmse(fv_rf,y_val)\n",
    "        errors = sum(l)\n",
    "        print(f\"Random Forest (n_estimators={n_estimators}, max_depth={max_depth}): {errors}\")\n",
    "        rmse_val_rf[f\"Random Forest (n_estimators={n_estimators}, max_depth={max_depth})\"] = errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost (n_estimators=50, max_depth=5): 2.2382036886596803\n",
      "XGBoost (n_estimators=50, max_depth=10): 2.4296844199774448\n",
      "XGBoost (n_estimators=50, max_depth=15): 2.4631091100936526\n",
      "XGBoost (n_estimators=100, max_depth=5): 2.238140190550444\n",
      "XGBoost (n_estimators=100, max_depth=10): 2.429684581840332\n",
      "XGBoost (n_estimators=100, max_depth=15): 2.463109209357113\n",
      "XGBoost (n_estimators=150, max_depth=5): 2.2381401952653035\n",
      "XGBoost (n_estimators=150, max_depth=10): 2.429684612408563\n",
      "XGBoost (n_estimators=150, max_depth=15): 2.4631091929351703\n"
     ]
    }
   ],
   "source": [
    "n_estimators_values = [50, 100, 150]\n",
    "max_depth_values = [5, 10, 15]\n",
    "rmse_val_xgboost = {}\n",
    "\n",
    "for n_estimators in n_estimators_values:\n",
    "    for max_depth in max_depth_values:\n",
    "        models, fv_train_xgboost = xgboost_regression(Xnormalizer.normalize_dataframe(x_train), \n",
    "                                                      Ynormalizer.normalize_dataframe(y_train),\n",
    "                                                      n_estimators, max_depth)\n",
    "        filenames = {col: f'xgboost_model_{col}.joblib' for col in models.keys()}\n",
    "        save_models(models, filenames)\n",
    "        fv_xgboost = Ynormalizer.denormalize_dataframe(load_and_predict(filenames, Xnormalizer.normalize_dataframe(x_val)))\n",
    "        l = rmse(fv_xgboost, y_val)\n",
    "        errors = sum(l)\n",
    "        print(f\"XGBoost (n_estimators={n_estimators}, max_depth={max_depth}): {errors}\")\n",
    "        rmse_val_xgboost[f\"XGBoost (n_estimators={n_estimators}, max_depth={max_depth})\"] = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extreme tree (n_estimators=50, max_depth=5): 2.2491290720350046\n",
      "extreme tree (n_estimators=50, max_depth=10): 2.1987711296706594\n",
      "extreme tree (n_estimators=50, max_depth=15): 2.249627069396408\n",
      "extreme tree (n_estimators=100, max_depth=5): 2.220702148710955\n",
      "extreme tree (n_estimators=100, max_depth=10): 2.1976942450302843\n",
      "extreme tree (n_estimators=100, max_depth=15): 2.21550871609382\n",
      "extreme tree (n_estimators=150, max_depth=5): 2.2120394032841486\n",
      "extreme tree (n_estimators=150, max_depth=10): 2.20275639486117\n",
      "extreme tree (n_estimators=150, max_depth=15): 2.2191943253198714\n"
     ]
    }
   ],
   "source": [
    "n_estimators_values = [50, 100, 150]\n",
    "max_depth_values = [5, 10, 15]\n",
    "rmse_val_extreme = {}\n",
    "feature_total={}\n",
    "\n",
    "for n_estimators in n_estimators_values:\n",
    "    for max_depth in max_depth_values:\n",
    "        models, fv_train_xtreme, features = extra_tree_regression(Xnormalizer.normalize_dataframe(x_train), \n",
    "                                                            Ynormalizer.normalize_dataframe(y_train),\n",
    "                                                            n_estimators, max_depth, return_feature_importance=True)\n",
    "        filenames = {col: f'Xtreme_tree_model_{col}.joblib' for col in models.keys()}\n",
    "        save_models(models, filenames)\n",
    "        fv_extreme = Ynormalizer.denormalize_dataframe(load_and_predict(filenames, Xnormalizer.normalize_dataframe(x_val)))\n",
    "        l = rmse(fv_extreme, y_val)\n",
    "        errors = sum(l)\n",
    "        print(f\"extreme tree (n_estimators={n_estimators}, max_depth={max_depth}): {errors}\")\n",
    "        rmse_val_extreme[f\"extreme tree (n_estimators={n_estimators}, max_depth={max_depth})\"] = errors\n",
    "        feature_total[f\"(n_estimators={n_estimators}, max_depth={max_depth})\"]=features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLS (n_components=2): 2.289964814291149\n",
      "PLS (n_components=5): 2.606140162571514\n",
      "PLS (n_components=10): 2.9000530053921487\n",
      "PLS (n_components=20): 3.473761572213263\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_components_values = [2, 5, 10, 20]\n",
    "rmse_val_pls = {}\n",
    "\n",
    "for n_components in n_components_values:\n",
    "    models, fv_train_pls = pls_regression(x_train, y_train, n_components)\n",
    "    filenames = {col: f'pls_model_{col}.joblib' for col in models.keys()}\n",
    "    save_models(models, filenames)\n",
    "    fv_pls = PLS_load_and_predict(filenames, x_val)\n",
    "    l=rmse(fv_pls, y_val)\n",
    "    errors = sum(l)\n",
    "    print(f\"PLS (n_components={n_components}): {errors}\")  \n",
    "    rmse_val_pls[f\"PLS (n_components={n_components})\"] = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "Neural Network (layer1=64, layer2=32): 2.641837778314605\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "Neural Network (layer1=64, layer2=64): 2.558155186175078\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "Neural Network (layer1=128, layer2=32): 2.692337470292018\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "Neural Network (layer1=128, layer2=64): 2.6573651175437143\n"
     ]
    }
   ],
   "source": [
    "rmse_val_nn = {}\n",
    "for layer1 in [64, 128]:\n",
    "    for layer2 in [32, 64]:\n",
    "        saved_filenames = {col: f'model_{col}.keras' for col in y_train.columns}\n",
    "        models = train_neural_network(Xnormalizer.normalize_dataframe(x_train), \n",
    "                                      Ynormalizer.normalize_dataframe(y_train), saved_filenames,layer1,layer2,epochs=10)\n",
    "        predicted_values_nn = Ynormalizer.denormalize_dataframe(load_and_predict_nn(saved_filenames, Xnormalizer.normalize_dataframe(x_val)))\n",
    "        l = rmse(predicted_values_nn, y_val)\n",
    "        errors = sum(l)\n",
    "        print(f\"Neural Network (layer1={layer1}, layer2={layer2}): {errors}\")\n",
    "        rmse_val_nn[f\"Neural Network (layer1={layer1}, layer2={layer2})\"] = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 471ms/step\n",
      "1/1 [==============================] - 0s 493ms/step\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "1/1 [==============================] - 0s 462ms/step\n",
      "1/1 [==============================] - 0s 466ms/step\n",
      "1/1 [==============================] - 0s 469ms/step\n",
      "1/1 [==============================] - 0s 462ms/step\n",
      "1/1 [==============================] - 0s 464ms/step\n",
      "1/1 [==============================] - 0s 465ms/step\n",
      "1/1 [==============================] - 0s 463ms/step\n",
      "1/1 [==============================] - 0s 469ms/step\n",
      "Neural Network LSTM (layer1=64, layer2=32, lstm=16): 2.079984467553925\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "1/1 [==============================] - 1s 722ms/step\n",
      "1/1 [==============================] - 1s 592ms/step\n",
      "1/1 [==============================] - 1s 642ms/step\n",
      "1/1 [==============================] - 1s 512ms/step\n",
      "1/1 [==============================] - 1s 517ms/step\n",
      "1/1 [==============================] - 0s 498ms/step\n",
      "1/1 [==============================] - 0s 497ms/step\n",
      "1/1 [==============================] - 0s 494ms/step\n",
      "1/1 [==============================] - 1s 516ms/step\n",
      "1/1 [==============================] - 0s 466ms/step\n",
      "Neural Network LSTM (layer1=64, layer2=32, lstm=32): 2.067332325619753\n",
      "1/1 [==============================] - 0s 474ms/step\n",
      "1/1 [==============================] - 0s 488ms/step\n",
      "1/1 [==============================] - 0s 462ms/step\n",
      "1/1 [==============================] - 0s 467ms/step\n",
      "1/1 [==============================] - 0s 463ms/step\n",
      "1/1 [==============================] - 0s 457ms/step\n",
      "1/1 [==============================] - 0s 461ms/step\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "1/1 [==============================] - 0s 469ms/step\n",
      "1/1 [==============================] - 0s 459ms/step\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "Neural Network LSTM (layer1=64, layer2=64, lstm=16): 2.037667557096569\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 463ms/step\n",
      "1/1 [==============================] - 0s 478ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 0s 465ms/step\n",
      "1/1 [==============================] - 0s 463ms/step\n",
      "1/1 [==============================] - 0s 461ms/step\n",
      "1/1 [==============================] - 0s 462ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 0s 459ms/step\n",
      "1/1 [==============================] - 0s 464ms/step\n",
      "Neural Network LSTM (layer1=64, layer2=64, lstm=32): 2.1235582180411257\n",
      "1/1 [==============================] - 0s 461ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 1s 629ms/step\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "1/1 [==============================] - 0s 461ms/step\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "1/1 [==============================] - 0s 459ms/step\n",
      "1/1 [==============================] - 0s 457ms/step\n",
      "1/1 [==============================] - 0s 499ms/step\n",
      "1/1 [==============================] - 1s 502ms/step\n",
      "1/1 [==============================] - 1s 626ms/step\n",
      "Neural Network LSTM (layer1=128, layer2=32, lstm=16): 1.993286089603307\n",
      "1/1 [==============================] - 0s 475ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 0s 483ms/step\n",
      "1/1 [==============================] - 0s 462ms/step\n",
      "1/1 [==============================] - 0s 466ms/step\n",
      "1/1 [==============================] - 0s 458ms/step\n",
      "1/1 [==============================] - 0s 464ms/step\n",
      "1/1 [==============================] - 0s 472ms/step\n",
      "1/1 [==============================] - 0s 488ms/step\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "1/1 [==============================] - 0s 462ms/step\n",
      "Neural Network LSTM (layer1=128, layer2=32, lstm=32): 1.9945123573861772\n",
      "1/1 [==============================] - 1s 526ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 1s 530ms/step\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "1/1 [==============================] - 0s 463ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 0s 465ms/step\n",
      "1/1 [==============================] - 0s 461ms/step\n",
      "1/1 [==============================] - 0s 463ms/step\n",
      "1/1 [==============================] - 0s 472ms/step\n",
      "1/1 [==============================] - 0s 461ms/step\n",
      "Neural Network LSTM (layer1=128, layer2=64, lstm=16): 1.9955528191882865\n",
      "1/1 [==============================] - 0s 461ms/step\n",
      "1/1 [==============================] - 0s 487ms/step\n",
      "1/1 [==============================] - 0s 485ms/step\n",
      "1/1 [==============================] - 0s 467ms/step\n",
      "1/1 [==============================] - 0s 457ms/step\n",
      "1/1 [==============================] - 0s 458ms/step\n",
      "1/1 [==============================] - 0s 461ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 0s 461ms/step\n",
      "1/1 [==============================] - 0s 461ms/step\n",
      "1/1 [==============================] - 0s 465ms/step\n",
      "Neural Network LSTM (layer1=128, layer2=64, lstm=32): 2.0773721159763157\n"
     ]
    }
   ],
   "source": [
    "rmse_val_lstm={}\n",
    "for layer1 in [64, 128]:\n",
    "    for layer2 in [32, 64]:\n",
    "        for lstm in [16,32]:\n",
    "            saved_filenames = {col: f'model_lstm_{col}.keras' for col in y_train.columns}\n",
    "            models = train_neural_LSTM(Xnormalizer.normalize_dataframe(x_train), \n",
    "                                    Ynormalizer.normalize_dataframe(y_train), saved_filenames, layer1, layer2,lstm, epochs=10)\n",
    "            predicted_values_nn_lstm = Ynormalizer.denormalize_dataframe(load_and_predict_nn(saved_filenames, Xnormalizer.normalize_dataframe(x_val)))\n",
    "            l = rmse(predicted_values_nn_lstm, y_val)\n",
    "            errors = sum(l)\n",
    "            print(f\"Neural Network LSTM (layer1={layer1}, layer2={layer2}, lstm={lstm}): {errors}\")\n",
    "            rmse_val_lstm[f\"Neural Network LSTM (layer1={layer1}, layer2={layer2}, lstm={lstm})\"] = errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "PLS+NN (n_components=2, layer1=64, layer2=32): nan\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "PLS+NN (n_components=2, layer1=64, layer2=64): nan\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "PLS+NN (n_components=2, layer1=128, layer2=32): nan\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "PLS+NN (n_components=2, layer1=128, layer2=64): nan\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "PLS+NN (n_components=5, layer1=64, layer2=32): nan\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "PLS+NN (n_components=5, layer1=64, layer2=64): nan\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "PLS+NN (n_components=5, layer1=128, layer2=32): nan\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "PLS+NN (n_components=5, layer1=128, layer2=64): nan\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "PLS+NN (n_components=10, layer1=64, layer2=32): nan\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "PLS+NN (n_components=10, layer1=64, layer2=64): nan\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "PLS+NN (n_components=10, layer1=128, layer2=32): nan\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "PLS+NN (n_components=10, layer1=128, layer2=64): nan\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "PLS+NN (n_components=20, layer1=64, layer2=32): nan\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "PLS+NN (n_components=20, layer1=64, layer2=64): nan\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "PLS+NN (n_components=20, layer1=128, layer2=32): nan\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "PLS+NN (n_components=20, layer1=128, layer2=64): nan\n"
     ]
    }
   ],
   "source": [
    "n_components_values = [2, 5, 10, 20]\n",
    "layer1_values = [64,128]\n",
    "layer2_values = [32,64]\n",
    "rmse_val_pls_nn = {}\n",
    "nn_pls_test={}\n",
    "for n_components in n_components_values:\n",
    "    for layer1 in layer1_values:\n",
    "        for layer2 in layer2_values:\n",
    "            nn_pls_pred_val, nn_pls_pred_test = nn_pls_model(Xnormalizer.normalize_dataframe(raw_x),\n",
    "                                                Ynormalizer.normalize_dataframe(raw_y),\n",
    "                                                n_components=n_components, \n",
    "                                                layer1=layer1, layer2=layer2)\n",
    "            nn_pls_pred_val=Ynormalizer.denormalize_dataframe(nn_pls_pred_val)\n",
    "            nn_pls_pred_test=Ynormalizer.denormalize_dataframe(nn_pls_pred_test)\n",
    "            rmse_pls_nn = rmse(nn_pls_pred_val, y_val)\n",
    "            test= rmse(nn_pls_pred_test, y_test)\n",
    "            errors = sum(rmse_pls_nn)\n",
    "            print(f\"PLS+NN (n_components={n_components}, layer1={layer1}, layer2={layer2}): {errors}\")\n",
    "            rmse_val_pls_nn[f\"PLS+NN (n_components={n_components}, layer1={layer1}, layer2={layer2})\"] = errors\n",
    "            nn_pls_test[f\"PLS+NN (n_components={n_components}, layer1={layer1}, layer2={layer2})\"] =test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we run all the regressions again but this time on optimized hyperparameters and \n",
    "x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe models are:\\n1- OLS\\n2- ARIMA\\n3- LASSO\\n4- Ridge\\n5- SGD\\n6- Random Forest\\n7- XGboost\\n8-PLS\\n9- SVM ?\\n10- Nueral Networks\\n11-LSTM\\n11- PLS+NN\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The models are:\n",
    "1- OLS\n",
    "2- ARIMA\n",
    "3- LASSO\n",
    "4- Ridge\n",
    "5- SGD\n",
    "6- Random Forest\n",
    "7- XGboost\n",
    "8-PLS\n",
    "9- SVM ?\n",
    "10- Nueral Networks\n",
    "11-LSTM\n",
    "11- PLS+NN\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ols</th>\n",
       "      <td>ols</td>\n",
       "      <td>17.580405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARIMA</th>\n",
       "      <td>ARIMA Forecast for (1, 0, 0)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lasso</th>\n",
       "      <td>Alpha: 0.1</td>\n",
       "      <td>2.037465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Alpha: 1.0</td>\n",
       "      <td>3.338673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sgd</th>\n",
       "      <td>Penalty: l1, Alpha: 0.1</td>\n",
       "      <td>1.957443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest (n_estimators=50, max_depth=15)</td>\n",
       "      <td>2.271086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost</th>\n",
       "      <td>XGBoost (n_estimators=100, max_depth=5)</td>\n",
       "      <td>2.23814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extreme_trees</th>\n",
       "      <td>extreme tree (n_estimators=100, max_depth=10)</td>\n",
       "      <td>2.197694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pls</th>\n",
       "      <td>PLS (n_components=2)</td>\n",
       "      <td>2.289965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn</th>\n",
       "      <td>Neural Network (layer1=64, layer2=64)</td>\n",
       "      <td>2.558155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm</th>\n",
       "      <td>Neural Network LSTM (layer1=128, layer2=32, ls...</td>\n",
       "      <td>1.993286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pls_nn</th>\n",
       "      <td>PLS+NN (n_components=2, layer1=64, layer2=32)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               0          1\n",
       "ols                                                          ols  17.580405\n",
       "ARIMA                               ARIMA Forecast for (1, 0, 0)        NaN\n",
       "lasso                                                 Alpha: 0.1   2.037465\n",
       "ridge                                                 Alpha: 1.0   3.338673\n",
       "sgd                                      Penalty: l1, Alpha: 0.1   1.957443\n",
       "rf                 Random Forest (n_estimators=50, max_depth=15)   2.271086\n",
       "xgboost                  XGBoost (n_estimators=100, max_depth=5)    2.23814\n",
       "extreme_trees      extreme tree (n_estimators=100, max_depth=10)   2.197694\n",
       "pls                                         PLS (n_components=2)   2.289965\n",
       "nn                         Neural Network (layer1=64, layer2=64)   2.558155\n",
       "lstm           Neural Network LSTM (layer1=128, layer2=32, ls...   1.993286\n",
       "pls_nn             PLS+NN (n_components=2, layer1=64, layer2=32)        NaN"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "validation_rmse_results={}\n",
    "\n",
    "validation_rmse_results['ols']=('ols',sum(rmse_ols))\n",
    "validation_rmse_results['ARIMA']=min(rmse_val_arima.items(), key=lambda x: x[1])\n",
    "validation_rmse_results['lasso']=min(rmse_val_lasso.items(), key=lambda x: x[1])\n",
    "validation_rmse_results['ridge']=min(rmse_val_ridge.items(), key=lambda x: x[1])\n",
    "validation_rmse_results['sgd']=min(rmse_val_sgd.items(), key=lambda x: x[1])\n",
    "validation_rmse_results['rf']=min(rmse_val_rf.items(), key=lambda x: x[1])\n",
    "validation_rmse_results['xgboost']=min(rmse_val_xgboost.items(), key=lambda x: x[1])\n",
    "validation_rmse_results['extreme_trees']=min(rmse_val_extreme.items(),key=lambda x: x[1])\n",
    "validation_rmse_results['pls']=min(rmse_val_pls.items(), key=lambda x: x[1])\n",
    "validation_rmse_results['nn']=min(rmse_val_nn.items(), key=lambda x: x[1])\n",
    "validation_rmse_results['lstm']=min(rmse_val_lstm.items(), key=lambda x: x[1])\n",
    "validation_rmse_results['pls_nn']=min(rmse_val_pls_nn.items(), key=lambda x: x[1])\n",
    "\n",
    "validation_rmse_results2=pd.DataFrame(validation_rmse_results)\n",
    "validation_rmse_results2=validation_rmse_results2.transpose()\n",
    "\n",
    "validation_rmse_results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "e:\\Users\\Alireza\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    }
   ],
   "source": [
    "order=(0,0,2)\n",
    "val_fitted_values, test_fitted_values = fit_arima_model(raw_y, train_percentage, val_percentage, test_percentage, order)\n",
    "rmse_test_arima = rmse(y_test, test_fitted_values)\n",
    "mae_test_arima=mae(y_test, test_fitted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder_name = \"modela\"\n",
    "alpha=0.1\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "models, fv_train_lasso = lasso_regression(Xnormalizer.normalize_dataframe(x_train),\n",
    "                                          Ynormalizer.normalize_dataframe(y_train), alpha)\n",
    "\n",
    "filenames = {col: os.path.join(folder_name, f'lasso_model_{col}.joblib') for col in models.keys()}\n",
    "save_models(models, filenames)\n",
    "# Calculate the RMSE on the test data\n",
    "ft_lasso = Ynormalizer.denormalize_dataframe(load_and_predict(filenames, Xnormalizer.normalize_dataframe(x_test)))\n",
    "rmse_test_lasso = rmse(y_test,ft_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory if it does not exist\n",
    "alpha=1.0\n",
    "\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "models, fv_train_ridge = ridge_regression(Xnormalizer.normalize_dataframe(x_train),\n",
    "                                          Ynormalizer.normalize_dataframe(y_train), alpha)\n",
    "\n",
    "filenames = {col: os.path.join(folder_name, f'ridge_model_{col}.joblib') for col in models.keys()}\n",
    "save_models(models, filenames)\n",
    "# Calculate the RMSE on the test data\n",
    "ft_ridge = Ynormalizer.denormalize_dataframe(load_and_predict(filenames, Xnormalizer.normalize_dataframe(x_test)))\n",
    "rmse_test_ridge = rmse(y_test, ft_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=1.0\n",
    "Penalty='elasticnet'\n",
    "\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "models, fv_train_sgd = sgd_regression(Xnormalizer.normalize_dataframe(x_train),\n",
    "                                      Ynormalizer.normalize_dataframe(y_train), penalty='elasticnet', alpha=alpha)\n",
    "\n",
    "filenames = {col: os.path.join(folder_name, f'sgd_model_{col}.joblib') for col in models.keys()}\n",
    "save_models(models, filenames)\n",
    "# Calculate the RMSE on the test data\n",
    "ft_sgd = Ynormalizer.denormalize_dataframe(load_and_predict(filenames, Xnormalizer.normalize_dataframe(x_test)))\n",
    "rmse_test_sgd = rmse(y_test, ft_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators=50\n",
    "max_depth=5\n",
    "\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "models, fv_train_random_forest = random_forest_regression(Xnormalizer.normalize_dataframe(x_train), \n",
    "                                                          Ynormalizer.normalize_dataframe(y_train), n_estimators, max_depth)\n",
    "filenames = {col: os.path.join(folder_name, f'random_forest_model_{col}.joblib') for col in models.keys()}\n",
    "save_models(models, filenames)\n",
    "# Calculate the RMSE on the test data\n",
    "ft_rf = Ynormalizer.denormalize_dataframe(load_and_predict(filenames, Xnormalizer.normalize_dataframe(x_test)))\n",
    "rmse_test_rf = rmse(y_test, ft_rf)\n",
    "mae_test_rf=mae(y_test, ft_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters for XGBoost\n",
    "n_estimators = 150\n",
    "max_depth = 5\n",
    "\n",
    "# Train the XGBoost model\n",
    "models, fv_train_xgboost = xgboost_regression(Xnormalizer.normalize_dataframe(x_train), \n",
    "                                              Ynormalizer.normalize_dataframe(y_train),\n",
    "                                              n_estimators, max_depth)\n",
    "\n",
    "# Save the trained models\n",
    "filenames = {col: f'xgboost_model_{col}.joblib' for col in models.keys()}\n",
    "save_models(models, filenames)\n",
    "\n",
    "# Calculate the predicted values on the test data\n",
    "ft_xgboost = Ynormalizer.denormalize_dataframe(load_and_predict(filenames, Xnormalizer.normalize_dataframe(x_test)))\n",
    "\n",
    "# Calculate the RMSE on the test data\n",
    "rmse_test_xgboost = rmse(y_test, ft_xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators=150\n",
    "max_depth=5\n",
    "\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "models, fv_train_extra_trees = extra_tree_regression(Xnormalizer.normalize_dataframe(x_train), \n",
    "                                                          Ynormalizer.normalize_dataframe(y_train), n_estimators, max_depth)\n",
    "filenames = {col: os.path.join(folder_name, f'Xtreme_tree_model_{col}.joblib') for col in models.keys()}\n",
    "save_models(models, filenames)\n",
    "# Calculate the RMSE on the test data\n",
    "ft_xt = Ynormalizer.denormalize_dataframe(load_and_predict(filenames, Xnormalizer.normalize_dataframe(x_test)))\n",
    "rmse_test_xt = rmse(y_test, ft_xt)\n",
    "mae_test_xt=mae(y_test, ft_xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run pls model\n",
    "n_components=2\n",
    "\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "models, fv_train_pls = pls_regression(x_train, y_train, n_components)\n",
    "filenames = {col: os.path.join(folder_name, f'pls_model_{col}.joblib') for col in models.keys()}\n",
    "save_models(models, filenames)\n",
    "# Calculate the RMSE on the test data\n",
    "ft_pls = PLS_load_and_predict(filenames, x_test)\n",
    "rmse_test_pls= rmse(y_test, ft_pls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# neurel network model\n",
    "layer1=64\n",
    "layer2=64\n",
    "\n",
    "# Run the neural network model\n",
    "saved_filenames = {col: f'model_{col}.keras' for col in y_train.columns}\n",
    "models = train_neural_network(Xnormalizer.normalize_dataframe(x_train), \n",
    "                              Ynormalizer.denormalize_dataframe(y_train), saved_filenames, layer1, layer2)\n",
    "predicted_values_nn = load_and_predict_nn(saved_filenames, Xnormalizer.normalize_dataframe(x_test))\n",
    "rmse_test_nn = rmse(y_test, predicted_values_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "# neurel network model\n",
    "layer1=64\n",
    "layer2=64\n",
    "lstm=16\n",
    "\n",
    "# Run the neural network model\n",
    "saved_filenames = {col: f'model_lstm_{col}.keras' for col in y_train.columns}\n",
    "models = train_neural_LSTM(Xnormalizer.normalize_dataframe(x_train), \n",
    "                              Ynormalizer.denormalize_dataframe(y_train), saved_filenames, layer1, layer2,lstm)\n",
    "predicted_values_nn_lstm = load_and_predict_nn_LSTM(saved_filenames, Xnormalizer.normalize_dataframe(x_test))\n",
    "rmse_test_nn_lstm = rmse(y_test, predicted_values_nn_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nn_components = 20\\nlayer1 = 128\\nlayer2 = 64\\n\\nnn_pls_test2=nn_pls_test[f\"PLS+NN (n_components={n_components}, layer1={layer1}, layer2={layer2})\"]\\nrmse_test_nn_pls=sum(nn_pls_test2) '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the PLS_nn algorithm on the test sample\n",
    "\"\"\"\n",
    "n_components = 20\n",
    "layer1 = 128\n",
    "layer2 = 64\n",
    "\n",
    "nn_pls_test2=nn_pls_test[f\"PLS+NN (n_components={n_components}, layer1={layer1}, layer2={layer2})\"]\n",
    "rmse_test_nn_pls=sum(nn_pls_test2) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final team\n",
    "test_rmse_results={}\n",
    "\n",
    "test_rmse_results['rw']=rmse_test_rw\n",
    "test_rmse_results['ols']=rmse_ols\n",
    "test_rmse_results['ARIMA']=rmse_test_arima\n",
    "test_rmse_results['lasso']=rmse_test_lasso\n",
    "test_rmse_results['ridge']=rmse_test_ridge\n",
    "test_rmse_results['sgd']=rmse_test_sgd\n",
    "test_rmse_results['rf']=rmse_test_rf\n",
    "test_rmse_results['xgboost']=rmse_test_xgboost\n",
    "test_rmse_results['extreme_trees']=rmse_test_xt\n",
    "test_rmse_results['pls']=rmse_test_pls\n",
    "test_rmse_results['nn']=rmse_test_nn\n",
    "test_rmse_results['lstm']=rmse_test_nn_lstm\n",
    "#test_rmse_results['pls_nn']=rmse_test_nn_pls\n",
    "\n",
    "test_rmse_results=pd.DataFrame(test_rmse_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Performing Hyperparameters</th>\n",
       "      <th>Performance in Validation Dataset (SMRSE)</th>\n",
       "      <th>Performance in Test Dataset (SMRSE)</th>\n",
       "      <th>Ranking Based on Performance on Test Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ols</th>\n",
       "      <td>ols</td>\n",
       "      <td>13.65</td>\n",
       "      <td>13.65</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARIMA</th>\n",
       "      <td>ARIMA Forecast for (2, 1, 0)</td>\n",
       "      <td>1.97</td>\n",
       "      <td>10.86</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lasso</th>\n",
       "      <td>Alpha: 0.1</td>\n",
       "      <td>2.04</td>\n",
       "      <td>10.86</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Alpha: 1.0</td>\n",
       "      <td>3.07</td>\n",
       "      <td>10.89</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sgd</th>\n",
       "      <td>Penalty: l1, Alpha: 0.1</td>\n",
       "      <td>1.93</td>\n",
       "      <td>10.88</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest (n_estimators=150, max_depth=5)</td>\n",
       "      <td>2.28</td>\n",
       "      <td>10.80</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost</th>\n",
       "      <td>XGBoost (n_estimators=50, max_depth=5)</td>\n",
       "      <td>2.27</td>\n",
       "      <td>10.84</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extreme_trees</th>\n",
       "      <td>extreme tree (n_estimators=50, max_depth=5)</td>\n",
       "      <td>2.28</td>\n",
       "      <td>10.73</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pls</th>\n",
       "      <td>PLS (n_components=2)</td>\n",
       "      <td>2.22</td>\n",
       "      <td>10.90</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn</th>\n",
       "      <td>Neural Network (layer1=128, layer2=64)</td>\n",
       "      <td>2.40</td>\n",
       "      <td>12.89</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm</th>\n",
       "      <td>Neural Network LSTM (layer1=64, layer2=64, lst...</td>\n",
       "      <td>1.95</td>\n",
       "      <td>12.50</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Best Performing Hyperparameters  \\\n",
       "ols                                                          ols   \n",
       "ARIMA                               ARIMA Forecast for (2, 1, 0)   \n",
       "lasso                                                 Alpha: 0.1   \n",
       "ridge                                                 Alpha: 1.0   \n",
       "sgd                                      Penalty: l1, Alpha: 0.1   \n",
       "rf                 Random Forest (n_estimators=150, max_depth=5)   \n",
       "xgboost                   XGBoost (n_estimators=50, max_depth=5)   \n",
       "extreme_trees        extreme tree (n_estimators=50, max_depth=5)   \n",
       "pls                                         PLS (n_components=2)   \n",
       "nn                        Neural Network (layer1=128, layer2=64)   \n",
       "lstm           Neural Network LSTM (layer1=64, layer2=64, lst...   \n",
       "\n",
       "               Performance in Validation Dataset (SMRSE)  \\\n",
       "ols                                                13.65   \n",
       "ARIMA                                               1.97   \n",
       "lasso                                               2.04   \n",
       "ridge                                               3.07   \n",
       "sgd                                                 1.93   \n",
       "rf                                                  2.28   \n",
       "xgboost                                             2.27   \n",
       "extreme_trees                                       2.28   \n",
       "pls                                                 2.22   \n",
       "nn                                                  2.40   \n",
       "lstm                                                1.95   \n",
       "\n",
       "               Performance in Test Dataset (SMRSE)  \\\n",
       "ols                                          13.65   \n",
       "ARIMA                                        10.86   \n",
       "lasso                                        10.86   \n",
       "ridge                                        10.89   \n",
       "sgd                                          10.88   \n",
       "rf                                           10.80   \n",
       "xgboost                                      10.84   \n",
       "extreme_trees                                10.73   \n",
       "pls                                          10.90   \n",
       "nn                                           12.89   \n",
       "lstm                                         12.50   \n",
       "\n",
       "               Ranking Based on Performance on Test Dataset  \n",
       "ols                                                    11.0  \n",
       "ARIMA                                                   4.0  \n",
       "lasso                                                   5.0  \n",
       "ridge                                                   7.0  \n",
       "sgd                                                     6.0  \n",
       "rf                                                      2.0  \n",
       "xgboost                                                 3.0  \n",
       "extreme_trees                                           1.0  \n",
       "pls                                                     8.0  \n",
       "nn                                                     10.0  \n",
       "lstm                                                    9.0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rmse_results_sum=pd.DataFrame(test_rmse_results.sum(axis=0))\n",
    "val_test_table=pd.merge(validation_rmse_results2,test_rmse_results_sum,left_index=True, right_index=True)\n",
    "\n",
    "val_test_table['Rank'] = val_test_table[\"0_y\"].rank(ascending=True)\n",
    "val_test_table[1]=pd.to_numeric(val_test_table[1])\n",
    "val_test_table.columns=[\"Best Performing Hyperparameters\", \"Performance in Validation Dataset (SMRSE)\" , \"Performance in Test Dataset (SMRSE)\" , \"Ranking Based on Performance on Test Dataset\"]\n",
    "\n",
    "val_test_table = val_test_table.round(2)\n",
    "val_test_table.to_csv(\"results/validation_test.csv\")\n",
    "val_test_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'mae_test_xt': mae_test_xt, 'mae_test_rf': mae_test_rf, 'mae_test_arima': mae_test_arima}\n",
    "df = pd.DataFrame(data)\n",
    "df.index=[\n",
    " \"2-year yield\",\n",
    " \"3-year yield\",\n",
    " \"5-year yield\",\n",
    "\"7-year yield\",\n",
    "\"10-year yield\",\n",
    "\"long-term yield\",\n",
    "\"1-month yield\",\n",
    "\"2-month yield\",\n",
    "\"3-month yield\",\n",
    "\"6-month yield\",\n",
    "\"1-year yield\"\n",
    "]\n",
    "\n",
    "df = df.round(2)  # Limit decimals to 2\n",
    "\n",
    "df.to_csv(\"results/mae_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rmse_results[[\"extreme_trees\",\"rf\", \"ARIMA\"]]\n",
    "\n",
    "test_rmse_results.index=[\n",
    " \"2-year yield\",\n",
    " \"3-year yield\",\n",
    " \"5-year yield\",\n",
    "\"7-year yield\",\n",
    "\"10-year yield\",\n",
    "\"long-term yield\",\n",
    "\"1-month yield\",\n",
    "\"2-month yield\",\n",
    "\"3-month yield\",\n",
    "\"6-month yield\",\n",
    "\"1-year yield\"\n",
    "]\n",
    "test_rmse_results = test_rmse_results.round(2)  # Limit decimals to 2\n",
    "\n",
    "test_rmse_results.to_csv(\"results/rmse_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# We make a dataframe which includes the performance of each alogirthm \n",
    "df_validation_rmse_results = pd.DataFrame(validation_rmse_results)\n",
    "df_validation_rmse_results_transposed = df_validation_rmse_results.transpose()\n",
    "merged_df=pd.merge(df_validation_rmse_results_transposed,\n",
    "pd.DataFrame(test_rmse_results.sum(axis=0)),left_index=True,right_index=True)\n",
    "\n",
    "merged_df.columns=['Best Performing Hyperparameters',\n",
    "'Performance in Validation Dataset (SMRSE)',\n",
    "'Performance in Test Dataset (SMRSE)']\n",
    "\n",
    "merged_df.to_csv(\"results/Table Performance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_xt.columns=[\n",
    " \"2-year yield\",\n",
    " \"3-year yield\",\n",
    " \"5-year yield\",\n",
    "\"7-year yield\",\n",
    "\"10-year yield\",\n",
    "\"long-term yield\",\n",
    "\"1-month yield\",\n",
    "\"2-month yield\",\n",
    "\"3-month yield\",\n",
    "\"6-month yield\",\n",
    "\"1-year yield\"\n",
    "]\n",
    "\n",
    "raw_initial_y.columns=ft_xt.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "ft_xt.index = pd.to_datetime(ft_xt.index)\n",
    "predictions = pd.DataFrame(index=raw_initial_y.index)\n",
    "\n",
    "# Iterate over each column in raw_initial_y\n",
    "for column in raw_initial_y.columns:\n",
    "    # Calculate the predictions by multiplying the forecast growth rate with the previous period's value\n",
    "    predictions[column] = raw_initial_y[column].shift(1) * (1 + ft_xt[column])\n",
    "\n",
    "# Drop the rows with NaN values\n",
    "predictions = predictions.dropna()\n",
    "\n",
    "# Merge predictions and raw_initial_y\n",
    "yield_forecast_performance_xt = pd.merge(predictions, raw_initial_y, left_index=True, right_index=True)\n",
    "\n",
    "# Add \"forecast\" to the beginning of each column name from predictions\n",
    "yield_forecast_performance_xt_shifted = yield_forecast_performance_xt.shift(1)\n",
    "\n",
    "# Display the merged dataframe\n",
    "yield_forecast_performance_xt_shifted.to_csv(\"results/performance.csv\")\n",
    "#print(yield_forecast_performance_xt.index[1],yield_forecast_performance_xt['1-month yield_x'][1],yield_forecast_performance_xt['1-month yield_y'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_forecast_performance_xt_shifted.columns=[\n",
    " \"Forecast: 2-year yield\",\n",
    " \"Forecast: 3-year yield\",\n",
    " \"Forecast: 5-year yield\",\n",
    "\"Forecast: 7-year yield\",\n",
    "\"Forecast: 10-year yield\",\n",
    "\"Forecast: long-term yield\",\n",
    "\"Forecast: 1-month yield\",\n",
    "\"Forecast: 2-month yield\",\n",
    "\"Forecast: 3-month yield\",\n",
    "\"Forecast: 6-month yield\",\n",
    "\"Forecast: 1-year yield\",\n",
    " \"2-year yield\",\n",
    " \"3-year yield\",\n",
    " \"5-year yield\",\n",
    "\"7-year yield\",\n",
    "\"10-year yield\",\n",
    "\"long-term yield\",\n",
    "\"1-month yield\",\n",
    "\"2-month yield\",\n",
    "\"3-month yield\",\n",
    "\"6-month yield\",\n",
    "\"1-year yield\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "def plot_timeseries(df1, df2, num_values,save_dir='plots'):\n",
    "    # Get the common column names\n",
    "    common_columns = df1.columns\n",
    "    # Create subplots\n",
    "    num_plots = len(common_columns)\n",
    "    \n",
    "    df1.index = pd.to_datetime(df1.index, format='%Y-%m')\n",
    "    df2.index = pd.to_datetime(df2.index, format='%Y-%m')\n",
    "    \n",
    "    # Plot each column in the subplots\n",
    "    for i, column in enumerate(common_columns):\n",
    "        fig, ax = plt.subplots(figsize=(9, 4.5))\n",
    "        ax.plot(df1.index[-num_values:], df1[column][-num_values:], label='Realized Yield', linestyle=\"--\")\n",
    "        ax.plot(df2.index[-num_values:], df2[column][-num_values:], label='Forecast')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Yield')\n",
    "        #ax.set_title(f'Realization vs Forecast for {column}')\n",
    "        ax.legend()\n",
    "\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=6))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "        x_min = min(df1.index[-num_values:].min(), df2.index[-num_values:].min())\n",
    "        x_max = max(df1.index[-num_values:].max(), df2.index[-num_values:].max())\n",
    "        ax.set_xlim([x_min, x_max])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        save_path = os.path.join(save_dir, f'{column}_timeseries_plot.jpeg')\n",
    "        fig.savefig(save_path)\n",
    "\n",
    "        plt.close()\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot_timeseries(raw_initial_y, \u001b[43mpredictions\u001b[49m, num_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m49\u001b[39m,save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/plots\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "plot_timeseries(raw_initial_y, predictions, num_values=49,save_dir='results/plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the dictionary to a pandas Series\n",
    "\n",
    "feature=feature_total['(n_estimators=150, max_depth=5)']\n",
    "\n",
    "feature_df = pd.DataFrame(feature)\n",
    "\n",
    "feature_df.columns=[\n",
    "\"1-month yield\",\n",
    "\"2-month yield\", \n",
    "\"3-month yield\",\n",
    "\"6-month yield\",\n",
    "\"1-year yield\",\n",
    "\"2-year yield\",\n",
    "\"3-year yield\",\n",
    "\"5-year yield\",\n",
    "\"7-year yield\",\n",
    "\"10-year yield\",\n",
    "\"Long Term yield\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import numpy as np\n",
    "\n",
    "top_n=590\n",
    "\n",
    "top_predictors = {}\n",
    "y_columns=feature_df.columns\n",
    "\n",
    "for y_col in y_columns:\n",
    "    # Define the dependent variable (target) for the current column\n",
    "    y = feature_df[y_col]\n",
    "\n",
    "\n",
    "    # Get feature importances\n",
    "    importances = feature_df[y_col]\n",
    "\n",
    "    # Sort the predictors by their importance\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Get the top n predictors and their importance scores\n",
    "    top_n_indices = indices[:top_n]\n",
    "    top_n_predictors = raw_x.columns[top_n_indices]\n",
    "    top_n_importances = importances[top_n_indices]\n",
    "\n",
    "    # Store results in dictionary\n",
    "    top_predictors[y_col] = list(zip(top_n_predictors, top_n_importances))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1-month yield': [('123263942', 0.10557984041655372),\n",
       "  ('Oil', 0.09845001840565236),\n",
       "  ('123263939', 0.060637572786842474),\n",
       "  ('39052', 0.04794689222356353),\n",
       "  ('39056', 0.04490152441513863),\n",
       "  ('39051', 0.04287770727470947),\n",
       "  ('37833', 0.041856263672106334),\n",
       "  ('39053', 0.03695756343198507),\n",
       "  ('39055', 0.03516447467527049),\n",
       "  ('39067', 0.03137773518422805),\n",
       "  ('39054', 0.028652580737179013),\n",
       "  ('2710140', 0.025979324838337627),\n",
       "  ('2710147', 0.024601816874000624),\n",
       "  ('39057', 0.02297402854018772),\n",
       "  ('120573154', 0.01941441383630767),\n",
       "  ('2710134', 0.019041257520891707),\n",
       "  ('TSX', 0.018825998723043608),\n",
       "  ('122535', 0.016575294515670892),\n",
       "  ('80691333', 0.015274546698939058),\n",
       "  ('123263908', 0.01289373518449095),\n",
       "  ('39066', 0.012646407770695464),\n",
       "  ('122534', 0.011482333382663776),\n",
       "  ('4429276', 0.0111960419571311),\n",
       "  ('123263945', 0.01087646624056543),\n",
       "  ('122536', 0.010685682871314345),\n",
       "  ('2710133', 0.010510367914372336),\n",
       "  ('122537', 0.010439035817014908),\n",
       "  ('2710148', 0.01032841187207352),\n",
       "  ('122142', 0.008977661827873953),\n",
       "  ('2710142', 0.008868117455498125),\n",
       "  ('39063', 0.00886365099332936),\n",
       "  ('52299983', 0.008740643995609875),\n",
       "  ('39065', 0.00827533711487752),\n",
       "  ('37810', 0.007910458041940991),\n",
       "  ('36462', 0.007315146094034583),\n",
       "  ('37832', 0.007099515330963771),\n",
       "  ('3469253', 0.006945309366403686),\n",
       "  ('80691336', 0.0069033148465713675),\n",
       "  ('39064', 0.0066205371267541255),\n",
       "  ('2710141', 0.005373993786304423),\n",
       "  ('2710150', 0.005121192768724261),\n",
       "  ('2710149', 0.004968657651842768),\n",
       "  ('122150', 0.00479114851030565),\n",
       "  ('122144', 0.0040532183502667175),\n",
       "  ('730416', 0.0039871702841579116),\n",
       "  ('31185443', 0.0037748191178617937),\n",
       "  ('80691334', 0.00376741480589702),\n",
       "  ('37805', 0.0037074439991499216),\n",
       "  ('37806', 0.0037070320582686684),\n",
       "  ('122550', 0.0034026263294396515),\n",
       "  ('37804', 0.0033668832506573067),\n",
       "  ('80691335', 0.0032754117312821926),\n",
       "  ('39078', 0.0028894123760325086),\n",
       "  ('37809', 0.0025918304124410156),\n",
       "  ('122148', 0.0022897385494435904),\n",
       "  ('80691332', 0.002119736370041016),\n",
       "  ('122141', 0.001714764504731976),\n",
       "  ('122530', 0.0016356687924816348),\n",
       "  ('36459', 0.0016098324024224368),\n",
       "  ('37808', 0.0014869980433891822),\n",
       "  ('36463', 0.001382712652529988),\n",
       "  ('62295562', 0.0012837345311474584),\n",
       "  ('39050', 0.001256053279918007),\n",
       "  ('122514', 0.0008913989196058297),\n",
       "  ('111955442', 0.0004340891297467933),\n",
       "  ('122140', 0.00024089470823240102),\n",
       "  ('39079', 0.0002090927088906682)],\n",
       " '2-month yield': [('123263942', 0.10797517659795748),\n",
       "  ('Oil', 0.07044729425645786),\n",
       "  ('123263939', 0.07015092521268927),\n",
       "  ('39056', 0.06367560517559293),\n",
       "  ('39053', 0.05109474136161087),\n",
       "  ('39055', 0.04534678237730316),\n",
       "  ('39051', 0.04404144020560958),\n",
       "  ('39052', 0.043942822660720186),\n",
       "  ('39054', 0.043597190784053254),\n",
       "  ('2710147', 0.025957525179531607),\n",
       "  ('2710140', 0.024761619464711497),\n",
       "  ('39057', 0.024702952980319312),\n",
       "  ('122534', 0.022353104813639616),\n",
       "  ('39067', 0.021828996256492785),\n",
       "  ('122536', 0.020986212885666104),\n",
       "  ('TSX', 0.020699207067601536),\n",
       "  ('122535', 0.019808273932239807),\n",
       "  ('37833', 0.01532361069329649),\n",
       "  ('2710134', 0.014143541458642462),\n",
       "  ('123263908', 0.01375466514780864),\n",
       "  ('123263945', 0.012967443533146555),\n",
       "  ('3469253', 0.012697732569184262),\n",
       "  ('2710142', 0.011381370353937303),\n",
       "  ('122537', 0.011142075926878896),\n",
       "  ('37810', 0.010756527340252531),\n",
       "  ('2710149', 0.009751063790513702),\n",
       "  ('39066', 0.009509130214740655),\n",
       "  ('122142', 0.009329066694316828),\n",
       "  ('2710148', 0.008563001321136184),\n",
       "  ('39064', 0.008556076104638852),\n",
       "  ('730416', 0.008498833348142505),\n",
       "  ('39065', 0.008352111607100554),\n",
       "  ('52299983', 0.008103596794041483),\n",
       "  ('80691333', 0.007483769069911089),\n",
       "  ('2710150', 0.007025296708138519),\n",
       "  ('4429276', 0.006554104266633922),\n",
       "  ('39063', 0.006424316568798699),\n",
       "  ('120573154', 0.006064748861100385),\n",
       "  ('2710141', 0.005418560610119459),\n",
       "  ('80691336', 0.005286358431550743),\n",
       "  ('37832', 0.0051746019716720415),\n",
       "  ('31185443', 0.004635046321871189),\n",
       "  ('122144', 0.004126339726975034),\n",
       "  ('80691335', 0.004113823875661986),\n",
       "  ('122141', 0.003952990271288145),\n",
       "  ('36462', 0.0035335999778649783),\n",
       "  ('39078', 0.0034391096430775366),\n",
       "  ('37806', 0.003216299427737195),\n",
       "  ('37809', 0.0030485139940144234),\n",
       "  ('2710133', 0.0028568685236080205),\n",
       "  ('37805', 0.0027555981615074105),\n",
       "  ('122514', 0.002710801416125515),\n",
       "  ('122148', 0.0021962636951863056),\n",
       "  ('37808', 0.0020097505436977373),\n",
       "  ('62295562', 0.0018368905606228),\n",
       "  ('36459', 0.0017866194887890437),\n",
       "  ('122530', 0.0017070987749786895),\n",
       "  ('80691334', 0.0014414234902363084),\n",
       "  ('39050', 0.0013615179890796838),\n",
       "  ('36463', 0.0012854459136541598),\n",
       "  ('122550', 0.001274333558047861),\n",
       "  ('37804', 0.0008660953766347501),\n",
       "  ('111955442', 0.0006999270018007718),\n",
       "  ('39079', 0.0006884132835262407),\n",
       "  ('122150', 0.0006611552453292683),\n",
       "  ('80691332', 0.0001645991407853625),\n",
       "  ('122140', 0.0)],\n",
       " '3-month yield': [('Oil', 0.07383203962317605),\n",
       "  ('39057', 0.061748188207936525),\n",
       "  ('123263942', 0.06169846045482405),\n",
       "  ('122536', 0.049678958613950226),\n",
       "  ('39056', 0.04507991464268269),\n",
       "  ('122535', 0.044443851025833286),\n",
       "  ('39052', 0.04282171798263738),\n",
       "  ('122534', 0.03787011380435713),\n",
       "  ('39051', 0.03641010292309032),\n",
       "  ('39055', 0.03545887030018386),\n",
       "  ('123263939', 0.035364691816840446),\n",
       "  ('39054', 0.03164052484384738),\n",
       "  ('2710147', 0.030738529397443405),\n",
       "  ('39053', 0.025257524480683916),\n",
       "  ('2710140', 0.023037377446091824),\n",
       "  ('3469253', 0.018918846167357013),\n",
       "  ('122537', 0.017548546371005967),\n",
       "  ('2710149', 0.01743456087983403),\n",
       "  ('TSX', 0.015173212070530892),\n",
       "  ('4429276', 0.014171900469507755),\n",
       "  ('39067', 0.013690121818141492),\n",
       "  ('123263908', 0.013018942812761072),\n",
       "  ('37810', 0.012814127881516615),\n",
       "  ('2710134', 0.012641895926318012),\n",
       "  ('120573154', 0.01257378263942377),\n",
       "  ('2710148', 0.012377077518242236),\n",
       "  ('123263945', 0.01202673808559216),\n",
       "  ('2710133', 0.011041674495248382),\n",
       "  ('2710142', 0.010716185269216584),\n",
       "  ('2710141', 0.009337759933070143),\n",
       "  ('730416', 0.008825195610834214),\n",
       "  ('39066', 0.008757351683540127),\n",
       "  ('80691336', 0.008360889992569502),\n",
       "  ('52299983', 0.008301247723847341),\n",
       "  ('80691335', 0.007866961854608208),\n",
       "  ('37832', 0.007624247708365215),\n",
       "  ('31185443', 0.007425746711064693),\n",
       "  ('122144', 0.006609009715609366),\n",
       "  ('2710150', 0.006566211963724175),\n",
       "  ('37833', 0.0064127874466907205),\n",
       "  ('122142', 0.0063138533404034055),\n",
       "  ('122141', 0.006118089105521276),\n",
       "  ('37805', 0.005556916289904935),\n",
       "  ('39063', 0.004962594339195027),\n",
       "  ('39064', 0.004930042459456378),\n",
       "  ('37809', 0.004872318571222646),\n",
       "  ('36462', 0.0047740793656927075),\n",
       "  ('37804', 0.004690630424175328),\n",
       "  ('80691334', 0.004596187219529667),\n",
       "  ('39065', 0.004134192872480425),\n",
       "  ('80691333', 0.003859481426208849),\n",
       "  ('122150', 0.0037596856176374456),\n",
       "  ('37806', 0.003645939680580415),\n",
       "  ('122148', 0.003169576241434244),\n",
       "  ('39050', 0.003072029227008381),\n",
       "  ('122514', 0.00244329777471382),\n",
       "  ('111955442', 0.0020818397024293927),\n",
       "  ('80691332', 0.0019253340246114443),\n",
       "  ('36463', 0.0018352186302864988),\n",
       "  ('62295562', 0.0018326201779218139),\n",
       "  ('37808', 0.0017359890309734081),\n",
       "  ('36459', 0.0014031541181978137),\n",
       "  ('122550', 0.0010871964617033623),\n",
       "  ('122530', 0.0009922039838776897),\n",
       "  ('39078', 0.0006808528374824137),\n",
       "  ('39079', 0.00021078876515302156),\n",
       "  ('122140', 0.0)],\n",
       " '6-month yield': [('39057', 0.07719988077315565),\n",
       "  ('37810', 0.07390622549063965),\n",
       "  ('Oil', 0.04571527401337534),\n",
       "  ('123263939', 0.04282329862435774),\n",
       "  ('39056', 0.042512490396399205),\n",
       "  ('122535', 0.040152481233561486),\n",
       "  ('3469253', 0.03996297934895922),\n",
       "  ('122534', 0.0376506909567919),\n",
       "  ('2710147', 0.03695384727399179),\n",
       "  ('122536', 0.03675247166587878),\n",
       "  ('123263942', 0.035578086923886525),\n",
       "  ('39051', 0.0324187006411099),\n",
       "  ('2710140', 0.0317894437045357),\n",
       "  ('39052', 0.030744678226713545),\n",
       "  ('39054', 0.029513201846386977),\n",
       "  ('39053', 0.024292167814707014),\n",
       "  ('2710142', 0.021279856153692305),\n",
       "  ('TSX', 0.020513440212696486),\n",
       "  ('39055', 0.017741076816412345),\n",
       "  ('122537', 0.015602673616690035),\n",
       "  ('123263945', 0.0145977980016953),\n",
       "  ('123263908', 0.01414167513468985),\n",
       "  ('39067', 0.013804364493790869),\n",
       "  ('4429276', 0.013209544002841078),\n",
       "  ('2710134', 0.013048062649168312),\n",
       "  ('2710149', 0.012946820954764657),\n",
       "  ('2710148', 0.012424545771190962),\n",
       "  ('52299983', 0.01155100205922996),\n",
       "  ('120573154', 0.011550496722325063),\n",
       "  ('730416', 0.009850983324228095),\n",
       "  ('80691335', 0.009229040924550257),\n",
       "  ('2710150', 0.008992449164905249),\n",
       "  ('2710133', 0.00862619473225492),\n",
       "  ('2710141', 0.008496328091281652),\n",
       "  ('31185443', 0.008050752709289469),\n",
       "  ('39066', 0.007111243731930666),\n",
       "  ('36462', 0.006194314993129155),\n",
       "  ('39065', 0.005898191867822867),\n",
       "  ('122514', 0.005780636636074094),\n",
       "  ('80691336', 0.0057724476933516174),\n",
       "  ('122142', 0.005769508981962915),\n",
       "  ('37832', 0.005657627219724255),\n",
       "  ('37809', 0.005385551740789523),\n",
       "  ('122150', 0.004769061668715565),\n",
       "  ('37805', 0.0036425006914511102),\n",
       "  ('39063', 0.003526117122549781),\n",
       "  ('122144', 0.003419879684547234),\n",
       "  ('37806', 0.0034044931580297327),\n",
       "  ('39050', 0.003369978744297376),\n",
       "  ('80691334', 0.003328768298025423),\n",
       "  ('80691333', 0.0032376494069640957),\n",
       "  ('39064', 0.0032148627021193095),\n",
       "  ('37833', 0.0029659997612381693),\n",
       "  ('37808', 0.0027025935441206835),\n",
       "  ('122141', 0.0019671397630432828),\n",
       "  ('62295562', 0.0016374356267363787),\n",
       "  ('36463', 0.0015887760600505417),\n",
       "  ('122148', 0.0015244828743812419),\n",
       "  ('39079', 0.0009267707608261732),\n",
       "  ('39078', 0.0008731431605451767),\n",
       "  ('122530', 0.0006382467879171567),\n",
       "  ('111955442', 0.0005828785683164358),\n",
       "  ('36459', 0.0004772848473723029),\n",
       "  ('37804', 0.0004672918178254928),\n",
       "  ('80691332', 0.000457460648296043),\n",
       "  ('122550', 8.463696769890333e-05),\n",
       "  ('122140', 0.0)],\n",
       " '1-year yield': [('37810', 0.07796951440414075),\n",
       "  ('39057', 0.06411721809610012),\n",
       "  ('123263939', 0.05462618403393764),\n",
       "  ('39056', 0.048247781568158475),\n",
       "  ('122534', 0.04459356269070525),\n",
       "  ('122536', 0.044205690737710364),\n",
       "  ('122535', 0.04375756200605004),\n",
       "  ('39052', 0.03724262046270019),\n",
       "  ('Oil', 0.035386832309886734),\n",
       "  ('39051', 0.034910053688562294),\n",
       "  ('2710147', 0.03420005763097822),\n",
       "  ('123263942', 0.03268431021893468),\n",
       "  ('39053', 0.027508208999277372),\n",
       "  ('39054', 0.024017374068122488),\n",
       "  ('2710140', 0.02211245691257435),\n",
       "  ('122537', 0.020791297296870067),\n",
       "  ('2710149', 0.020376919125043642),\n",
       "  ('4429276', 0.019773364072065612),\n",
       "  ('3469253', 0.01894672342962585),\n",
       "  ('TSX', 0.018928229249324097),\n",
       "  ('123263908', 0.0186427301340243),\n",
       "  ('39055', 0.018062571608424477),\n",
       "  ('2710148', 0.016049076107776627),\n",
       "  ('2710142', 0.015769756130547978),\n",
       "  ('730416', 0.014137974887612693),\n",
       "  ('2710150', 0.013273032887707743),\n",
       "  ('36462', 0.011237276037548318),\n",
       "  ('123263945', 0.010737171956186784),\n",
       "  ('2710134', 0.010491571246273956),\n",
       "  ('122144', 0.010013288237319597),\n",
       "  ('2710141', 0.0086935576451355),\n",
       "  ('52299983', 0.008571322931605618),\n",
       "  ('39067', 0.008497109048500023),\n",
       "  ('2710133', 0.008389453718577392),\n",
       "  ('80691335', 0.007930013973135877),\n",
       "  ('122141', 0.007710912807058976),\n",
       "  ('120573154', 0.007299350512115935),\n",
       "  ('122142', 0.006659786591383536),\n",
       "  ('39050', 0.005178747311525301),\n",
       "  ('39066', 0.004870273005217695),\n",
       "  ('39065', 0.004693326754201966),\n",
       "  ('37832', 0.004493854830397641),\n",
       "  ('122150', 0.004165443203123355),\n",
       "  ('31185443', 0.004141616536557269),\n",
       "  ('80691336', 0.00411640158713596),\n",
       "  ('37805', 0.0038591713950618827),\n",
       "  ('80691333', 0.0036516481308291287),\n",
       "  ('37806', 0.0034374635015138933),\n",
       "  ('37809', 0.0034316903814052137),\n",
       "  ('36463', 0.003428926801592863),\n",
       "  ('37833', 0.0034253452596798473),\n",
       "  ('39064', 0.0033534433836311505),\n",
       "  ('80691334', 0.0024590488274846634),\n",
       "  ('39063', 0.0022443780307334718),\n",
       "  ('122514', 0.0021956906860090687),\n",
       "  ('122148', 0.0020732768070086402),\n",
       "  ('122550', 0.0014568040498465237),\n",
       "  ('80691332', 0.001390441526929568),\n",
       "  ('37808', 0.001229963159020778),\n",
       "  ('122530', 0.0011803520955301493),\n",
       "  ('36459', 0.0011297635492856044),\n",
       "  ('111955442', 0.0005532439836868568),\n",
       "  ('122140', 0.0004616668390602641),\n",
       "  ('37804', 0.0003465798427630458),\n",
       "  ('39079', 0.00025334222440407403),\n",
       "  ('62295562', 0.00012493248502686336),\n",
       "  ('39078', 9.124634963369766e-05)],\n",
       " '2-year yield': [('37810', 0.11006354496857955),\n",
       "  ('39057', 0.09593060053968856),\n",
       "  ('123263939', 0.09037884854202512),\n",
       "  ('2710140', 0.04593957807841473),\n",
       "  ('122535', 0.045436234683772045),\n",
       "  ('Oil', 0.04339247605401508),\n",
       "  ('122536', 0.04069992330274575),\n",
       "  ('39056', 0.03360207414293776),\n",
       "  ('122534', 0.03329537586173452),\n",
       "  ('39052', 0.02325287387175046),\n",
       "  ('39054', 0.022413591320943085),\n",
       "  ('123263942', 0.02135890184156448),\n",
       "  ('4429276', 0.021249906673318535),\n",
       "  ('39051', 0.020629317184414107),\n",
       "  ('3469253', 0.020291813735019607),\n",
       "  ('2710147', 0.01851610751090119),\n",
       "  ('730416', 0.01823794789735394),\n",
       "  ('123263945', 0.01821721789744367),\n",
       "  ('122537', 0.01602666659305986),\n",
       "  ('2710148', 0.015534259409070668),\n",
       "  ('2710149', 0.014358129389280203),\n",
       "  ('36462', 0.012540440653401739),\n",
       "  ('2710142', 0.012170480187832972),\n",
       "  ('TSX', 0.011725132777362327),\n",
       "  ('39053', 0.011660595613124767),\n",
       "  ('2710150', 0.01136926402878535),\n",
       "  ('2710141', 0.010443682151247989),\n",
       "  ('2710134', 0.009666752911897572),\n",
       "  ('39055', 0.009445220740168063),\n",
       "  ('2710133', 0.009080279956285135),\n",
       "  ('122150', 0.008608000946069226),\n",
       "  ('52299983', 0.008360895484498655),\n",
       "  ('31185443', 0.007294969259937558),\n",
       "  ('120573154', 0.00711929272976087),\n",
       "  ('80691334', 0.007095652951005597),\n",
       "  ('122144', 0.006322824611731479),\n",
       "  ('37832', 0.006071902244874728),\n",
       "  ('80691335', 0.005899591742365758),\n",
       "  ('39067', 0.005637891652049583),\n",
       "  ('123263908', 0.005507069251745173),\n",
       "  ('122142', 0.005457698412926323),\n",
       "  ('39064', 0.004722368728259382),\n",
       "  ('39063', 0.004473893879111101),\n",
       "  ('39050', 0.004397031294430742),\n",
       "  ('39066', 0.004375368485119382),\n",
       "  ('39065', 0.004275752822129654),\n",
       "  ('122141', 0.004254260793572535),\n",
       "  ('80691333', 0.0036276681866828095),\n",
       "  ('37809', 0.0032191927401238234),\n",
       "  ('37805', 0.0031692125718601996),\n",
       "  ('80691336', 0.002822506535934908),\n",
       "  ('122514', 0.002621034540183311),\n",
       "  ('36459', 0.0025447977179254666),\n",
       "  ('37806', 0.0023792991411855573),\n",
       "  ('122530', 0.0020584894067929454),\n",
       "  ('37804', 0.0020508918936696895),\n",
       "  ('62295562', 0.0020216406232998795),\n",
       "  ('39079', 0.0015665803240951424),\n",
       "  ('36463', 0.0012780672503661034),\n",
       "  ('111955442', 0.0008484962275170793),\n",
       "  ('80691332', 0.0008483631939928005),\n",
       "  ('37808', 0.000844682711939092),\n",
       "  ('37833', 0.0007983625029014876),\n",
       "  ('122140', 0.00034735818386222924),\n",
       "  ('39078', 6.443391659061826e-05),\n",
       "  ('122148', 4.868255387915343e-05),\n",
       "  ('122550', 3.8535969471282424e-05)],\n",
       " '3-year yield': [('2710134', 0.0999196120862902),\n",
       "  ('123263939', 0.08006613256694704),\n",
       "  ('122537', 0.07927347053408133),\n",
       "  ('123263908', 0.06114833335367815),\n",
       "  ('122144', 0.05162178512639409),\n",
       "  ('2710140', 0.042543197094528154),\n",
       "  ('3469253', 0.04195884976030512),\n",
       "  ('Oil', 0.037446436548623445),\n",
       "  ('39063', 0.02834844978669817),\n",
       "  ('80691335', 0.0255571225699981),\n",
       "  ('39057', 0.02196317201517228),\n",
       "  ('TSX', 0.021177480966757183),\n",
       "  ('52299983', 0.018460462094911968),\n",
       "  ('31185443', 0.01701850846276405),\n",
       "  ('122535', 0.01575523172008976),\n",
       "  ('80691334', 0.015580749511055987),\n",
       "  ('122141', 0.014313457761269714),\n",
       "  ('39065', 0.013983951549791087),\n",
       "  ('122536', 0.013545640147386736),\n",
       "  ('122534', 0.013400615085404221),\n",
       "  ('730416', 0.013053541424087567),\n",
       "  ('39052', 0.012768610967477502),\n",
       "  ('80691333', 0.012067862193849574),\n",
       "  ('2710149', 0.011395468146009972),\n",
       "  ('39066', 0.011322215934525722),\n",
       "  ('122150', 0.010784811655358505),\n",
       "  ('2710148', 0.010617966948545075),\n",
       "  ('2710147', 0.010458180956890889),\n",
       "  ('39050', 0.010088798778047175),\n",
       "  ('2710142', 0.009928630821856495),\n",
       "  ('122514', 0.009468231727530365),\n",
       "  ('39053', 0.00945856597004578),\n",
       "  ('39054', 0.009408563091644681),\n",
       "  ('39064', 0.0093303616441058),\n",
       "  ('39055', 0.009324446039611413),\n",
       "  ('122142', 0.009277095876798196),\n",
       "  ('2710150', 0.009136970051577518),\n",
       "  ('39051', 0.00912434848486878),\n",
       "  ('39078', 0.007988843635932937),\n",
       "  ('2710133', 0.007139858017165032),\n",
       "  ('39079', 0.006746568690079768),\n",
       "  ('4429276', 0.006711502987863831),\n",
       "  ('2710141', 0.006396538052442231),\n",
       "  ('39056', 0.006198645438940352),\n",
       "  ('123263945', 0.005934930301294383),\n",
       "  ('37832', 0.005312761059261863),\n",
       "  ('80691336', 0.004865288866157195),\n",
       "  ('39067', 0.004706542406141685),\n",
       "  ('36459', 0.004589277636179028),\n",
       "  ('120573154', 0.0035911651287448823),\n",
       "  ('36462', 0.003496041465303824),\n",
       "  ('37805', 0.0034895212942670623),\n",
       "  ('37809', 0.0034305125340000162),\n",
       "  ('37810', 0.0030492928359795515),\n",
       "  ('62295562', 0.0023354172151817793),\n",
       "  ('111955442', 0.0023187914324975267),\n",
       "  ('37833', 0.0019652430013686385),\n",
       "  ('123263942', 0.0018326246654570557),\n",
       "  ('37808', 0.001730694503549832),\n",
       "  ('122550', 0.0017124528541617708),\n",
       "  ('37804', 0.0015860247128628813),\n",
       "  ('37806', 0.0010395143359672252),\n",
       "  ('80691332', 0.0009348352271650826),\n",
       "  ('122530', 0.0006587014780186888),\n",
       "  ('36463', 0.00014108076903822245),\n",
       "  ('122148', 0.0),\n",
       "  ('122140', 0.0)],\n",
       " '5-year yield': [('123263939', 0.0887023464782446),\n",
       "  ('52299983', 0.08287576030979174),\n",
       "  ('123263908', 0.06642497439835554),\n",
       "  ('122537', 0.05486879955832039),\n",
       "  ('Oil', 0.05219603198568758),\n",
       "  ('31185443', 0.04853089546829619),\n",
       "  ('2710140', 0.04612400678094665),\n",
       "  ('80691335', 0.04369498840326395),\n",
       "  ('2710134', 0.040578159789499045),\n",
       "  ('TSX', 0.03977137628828866),\n",
       "  ('3469253', 0.022403115411988223),\n",
       "  ('122536', 0.022034702799590498),\n",
       "  ('80691333', 0.02108401111149306),\n",
       "  ('122535', 0.020854589526120793),\n",
       "  ('80691334', 0.020040999458070766),\n",
       "  ('39055', 0.019344203988830907),\n",
       "  ('122534', 0.01934085110791728),\n",
       "  ('122144', 0.016859643597661227),\n",
       "  ('39057', 0.016590275370387916),\n",
       "  ('2710148', 0.012999475586537619),\n",
       "  ('39063', 0.012295875299991413),\n",
       "  ('39054', 0.012095997637644093),\n",
       "  ('39052', 0.01199561850585332),\n",
       "  ('39064', 0.011245138139107167),\n",
       "  ('80691336', 0.01103166901251411),\n",
       "  ('39051', 0.009654144246046948),\n",
       "  ('39065', 0.009627440754080334),\n",
       "  ('122141', 0.009165833321169057),\n",
       "  ('4429276', 0.009146686843210393),\n",
       "  ('2710147', 0.009022192450109311),\n",
       "  ('39066', 0.008108615364772379),\n",
       "  ('123263942', 0.007882519511740415),\n",
       "  ('39053', 0.007565758362336138),\n",
       "  ('2710149', 0.007061995051137637),\n",
       "  ('123263945', 0.006612397420124076),\n",
       "  ('2710141', 0.00621290889846643),\n",
       "  ('2710142', 0.006102469071604054),\n",
       "  ('2710150', 0.006036160945978609),\n",
       "  ('122150', 0.006013431533332198),\n",
       "  ('120573154', 0.005849239035528384),\n",
       "  ('39067', 0.00546833745508718),\n",
       "  ('39056', 0.005188300260948382),\n",
       "  ('2710133', 0.004922938303524965),\n",
       "  ('37810', 0.004588940473989178),\n",
       "  ('39050', 0.004559952810887112),\n",
       "  ('122550', 0.004074133793270138),\n",
       "  ('122514', 0.004054890654470127),\n",
       "  ('80691332', 0.00378175560689035),\n",
       "  ('37804', 0.0034805603055306352),\n",
       "  ('122142', 0.0034713197458664143),\n",
       "  ('37805', 0.003085126638904526),\n",
       "  ('122530', 0.0030126861252974826),\n",
       "  ('37808', 0.0024351346681642785),\n",
       "  ('37832', 0.0023135865147630697),\n",
       "  ('39079', 0.002013947260229829),\n",
       "  ('39078', 0.0019640008598184105),\n",
       "  ('37809', 0.0019305293399826133),\n",
       "  ('36459', 0.0019176534205806888),\n",
       "  ('37833', 0.0018455682306176824),\n",
       "  ('62295562', 0.0014323268771810709),\n",
       "  ('37806', 0.00138827923872246),\n",
       "  ('730416', 0.001301762571754516),\n",
       "  ('111955442', 0.0011489176237163155),\n",
       "  ('122148', 0.0008694126380921623),\n",
       "  ('36463', 0.0008548938992130767),\n",
       "  ('36462', 0.0008497458584582608),\n",
       "  ('122140', 0.0)],\n",
       " '7-year yield': [('52299983', 0.08958903169822795),\n",
       "  ('Oil', 0.0875279665772665),\n",
       "  ('123263939', 0.08647180033742309),\n",
       "  ('123263908', 0.0757827129428825),\n",
       "  ('31185443', 0.0462168712391784),\n",
       "  ('2710140', 0.042468930563222144),\n",
       "  ('80691335', 0.03671330247110859),\n",
       "  ('122537', 0.033301240269151926),\n",
       "  ('TSX', 0.027353688552073475),\n",
       "  ('123263942', 0.026174040428921824),\n",
       "  ('122536', 0.02590969503144144),\n",
       "  ('3469253', 0.02566257048869649),\n",
       "  ('2710147', 0.021681930662597333),\n",
       "  ('2710149', 0.021603393951613616),\n",
       "  ('122535', 0.019143393747868963),\n",
       "  ('39057', 0.018997515809261508),\n",
       "  ('2710134', 0.01806577354305953),\n",
       "  ('39055', 0.017781290555304852),\n",
       "  ('39063', 0.014658787086432541),\n",
       "  ('122534', 0.014019328542491239),\n",
       "  ('80691336', 0.013712077783715517),\n",
       "  ('39051', 0.013358496948551946),\n",
       "  ('80691334', 0.013331215516770073),\n",
       "  ('4429276', 0.01332005364378249),\n",
       "  ('80691333', 0.013269640760063366),\n",
       "  ('2710148', 0.012480635835261903),\n",
       "  ('2710150', 0.011529809286866557),\n",
       "  ('39054', 0.011346448286383865),\n",
       "  ('39052', 0.010896052680468797),\n",
       "  ('39067', 0.010793992412733976),\n",
       "  ('39065', 0.009247076094667324),\n",
       "  ('39064', 0.009246314793013721),\n",
       "  ('2710142', 0.007707788585189968),\n",
       "  ('120573154', 0.007305785628012789),\n",
       "  ('123263945', 0.006856888152697867),\n",
       "  ('122142', 0.0062083727241339925),\n",
       "  ('122141', 0.006115968166037642),\n",
       "  ('39056', 0.005545748459958219),\n",
       "  ('39066', 0.0054823085338092685),\n",
       "  ('37833', 0.0053932341065337),\n",
       "  ('39050', 0.004990117603951319),\n",
       "  ('39053', 0.00475449404289023),\n",
       "  ('122144', 0.004534996622668171),\n",
       "  ('122514', 0.004327243666101493),\n",
       "  ('122150', 0.003846314803163467),\n",
       "  ('37810', 0.0033845360649351693),\n",
       "  ('37805', 0.0032016496359138194),\n",
       "  ('122530', 0.0031552315980543973),\n",
       "  ('2710141', 0.002905739429955866),\n",
       "  ('2710133', 0.0024545572801997076),\n",
       "  ('39078', 0.0024054974373883277),\n",
       "  ('37806', 0.0022771887218444612),\n",
       "  ('80691332', 0.0022720268485643897),\n",
       "  ('62295562', 0.002077060079183695),\n",
       "  ('730416', 0.0017335179410197254),\n",
       "  ('36462', 0.0016145650185441325),\n",
       "  ('122550', 0.0015045996362583489),\n",
       "  ('111955442', 0.0010785955694941373),\n",
       "  ('37832', 0.00096820067692387),\n",
       "  ('37808', 0.0009265193908123315),\n",
       "  ('39079', 0.0009177052165725435),\n",
       "  ('37804', 0.0006605911291005449),\n",
       "  ('36463', 0.0005461390018447774),\n",
       "  ('36459', 0.0005369275428378789),\n",
       "  ('122148', 0.00033770360156828023),\n",
       "  ('37809', 0.0003171085433320102),\n",
       "  ('122140', 0.0)],\n",
       " '10-year yield': [('123263939', 0.09247256821517279),\n",
       "  ('123263908', 0.052471317552881626),\n",
       "  ('Oil', 0.050098481605630706),\n",
       "  ('2710134', 0.04780232606239468),\n",
       "  ('52299983', 0.04413911987406274),\n",
       "  ('123263942', 0.043917801471505466),\n",
       "  ('2710140', 0.04185200601361183),\n",
       "  ('31185443', 0.0378455819351175),\n",
       "  ('3469253', 0.030606681465638295),\n",
       "  ('80691335', 0.02414144968428014),\n",
       "  ('120573154', 0.02359984628667068),\n",
       "  ('122537', 0.023321471062253928),\n",
       "  ('2710147', 0.022455412159975253),\n",
       "  ('122535', 0.020791147106683807),\n",
       "  ('2710149', 0.020694813090987137),\n",
       "  ('39057', 0.02017125770934054),\n",
       "  ('TSX', 0.019362687719394554),\n",
       "  ('122536', 0.01887633433741236),\n",
       "  ('39052', 0.018002742732655532),\n",
       "  ('80691333', 0.017426044369716013),\n",
       "  ('39067', 0.017325794569143037),\n",
       "  ('122534', 0.016815561376024662),\n",
       "  ('39051', 0.016714005820984958),\n",
       "  ('122550', 0.015507620230196757),\n",
       "  ('39065', 0.015323118337963548),\n",
       "  ('2710141', 0.01388840205646245),\n",
       "  ('122141', 0.013772941310229337),\n",
       "  ('39064', 0.012399635492015922),\n",
       "  ('2710133', 0.011581648592464834),\n",
       "  ('2710148', 0.011244559138949258),\n",
       "  ('39056', 0.01043823154308058),\n",
       "  ('123263945', 0.010276262006808118),\n",
       "  ('39054', 0.01025481437033612),\n",
       "  ('2710150', 0.009477179888036223),\n",
       "  ('39063', 0.009446232185165776),\n",
       "  ('122514', 0.009211961333386138),\n",
       "  ('2710142', 0.00895404442940878),\n",
       "  ('39053', 0.00877107590446092),\n",
       "  ('80691334', 0.008317028880263602),\n",
       "  ('80691336', 0.007399880333198456),\n",
       "  ('39055', 0.007309829196259446),\n",
       "  ('37833', 0.006812377207514089),\n",
       "  ('39050', 0.0062830209403632676),\n",
       "  ('39066', 0.00562457919564216),\n",
       "  ('122530', 0.0051042476975100125),\n",
       "  ('122144', 0.0050483554552656065),\n",
       "  ('39079', 0.004984607641190765),\n",
       "  ('80691332', 0.004929273125227231),\n",
       "  ('4429276', 0.004636860687260152),\n",
       "  ('39078', 0.004495729784040906),\n",
       "  ('122142', 0.004356028824371916),\n",
       "  ('37810', 0.0036678591166659142),\n",
       "  ('37804', 0.0032973886434869842),\n",
       "  ('37809', 0.0032882650174252265),\n",
       "  ('730416', 0.003184063611986388),\n",
       "  ('37832', 0.002875205102408873),\n",
       "  ('122150', 0.0027809086671586277),\n",
       "  ('111955442', 0.0024192230942404807),\n",
       "  ('37808', 0.0023840766500526633),\n",
       "  ('37806', 0.0021608586725429106),\n",
       "  ('62295562', 0.0018430319849189055),\n",
       "  ('36459', 0.0014844441958571843),\n",
       "  ('122148', 0.0013757805650113203),\n",
       "  ('36462', 0.0010250671887628432),\n",
       "  ('36463', 0.0008352829631294555),\n",
       "  ('37805', 0.0006245465177416004),\n",
       "  ('122140', 0.0)],\n",
       " 'Long Term yield': [('Oil', 0.11911608752291405),\n",
       "  ('123263939', 0.11480696227252458),\n",
       "  ('123263942', 0.04864668248245352),\n",
       "  ('2710140', 0.0446927766380585),\n",
       "  ('39067', 0.03862212278238173),\n",
       "  ('120573154', 0.03285609762863543),\n",
       "  ('122537', 0.030558855428977127),\n",
       "  ('3469253', 0.02508198647019755),\n",
       "  ('2710147', 0.024205601944439718),\n",
       "  ('122550', 0.024129654244155634),\n",
       "  ('39051', 0.024107605315699256),\n",
       "  ('39052', 0.022620494496515917),\n",
       "  ('39066', 0.021428550521734507),\n",
       "  ('2710134', 0.02125739525499868),\n",
       "  ('123263908', 0.02015318426803879),\n",
       "  ('39064', 0.019256994612514275),\n",
       "  ('39057', 0.018953664355377806),\n",
       "  ('122530', 0.017836412255544637),\n",
       "  ('TSX', 0.01728901492518361),\n",
       "  ('39056', 0.017075788178833276),\n",
       "  ('39065', 0.017009896304579598),\n",
       "  ('39053', 0.016710307632319784),\n",
       "  ('31185443', 0.013539710581545074),\n",
       "  ('122534', 0.012973040700727407),\n",
       "  ('80691333', 0.012397502919733268),\n",
       "  ('39078', 0.011664679052221915),\n",
       "  ('122514', 0.011427776090128405),\n",
       "  ('2710148', 0.01092877886047024),\n",
       "  ('39079', 0.010486319137003678),\n",
       "  ('39054', 0.010027179707824628),\n",
       "  ('39063', 0.009740588361983254),\n",
       "  ('39055', 0.009685333066298906),\n",
       "  ('122536', 0.009450050162529266),\n",
       "  ('39050', 0.008587927244439493),\n",
       "  ('2710142', 0.0075660856175772525),\n",
       "  ('2710150', 0.00743830135114261),\n",
       "  ('80691335', 0.007362285391464122),\n",
       "  ('80691336', 0.007149085364095391),\n",
       "  ('2710149', 0.006736302192095523),\n",
       "  ('80691334', 0.006415582186455456),\n",
       "  ('122150', 0.006105923520878334),\n",
       "  ('4429276', 0.005863273910073869),\n",
       "  ('123263945', 0.005820387571402528),\n",
       "  ('122144', 0.0054947946556933224),\n",
       "  ('2710133', 0.005388313162041684),\n",
       "  ('122142', 0.005314627854148641),\n",
       "  ('122535', 0.0052061833482162),\n",
       "  ('2710141', 0.004987108543492528),\n",
       "  ('52299983', 0.004581640148569257),\n",
       "  ('80691332', 0.004381901957921455),\n",
       "  ('122141', 0.0042227738054432225),\n",
       "  ('37809', 0.003829118046395796),\n",
       "  ('37832', 0.0037094414846567648),\n",
       "  ('37810', 0.003504896943267822),\n",
       "  ('37804', 0.003132602921418258),\n",
       "  ('37833', 0.002681527792137045),\n",
       "  ('37805', 0.002550850709021414),\n",
       "  ('37806', 0.002532372002806339),\n",
       "  ('730416', 0.002381745056437106),\n",
       "  ('62295562', 0.00233933497424969),\n",
       "  ('36462', 0.0017349088635556327),\n",
       "  ('36463', 0.0013355607612520655),\n",
       "  ('111955442', 0.0010096946938983858),\n",
       "  ('37808', 0.0010017874344551924),\n",
       "  ('122148', 0.0005396909407134982),\n",
       "  ('36459', 0.0003568753740402171),\n",
       "  ('122140', 0.0)]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_predictors\n",
    "aggregated_top_predictors = {}\n",
    "\n",
    "for key, value in top_predictors.items():\n",
    "    aggregated_values = {}\n",
    "    for variable, importance in value:\n",
    "        variable_name = variable.split(\"_\")[1]\n",
    "        if variable_name not in aggregated_values:\n",
    "            aggregated_values[variable_name] = importance\n",
    "        else:\n",
    "            aggregated_values[variable_name] += importance\n",
    "    sorted_values = sorted(aggregated_values.items(), key=lambda x: x[1], reverse=True)\n",
    "    aggregated_top_predictors[key] = sorted_values\n",
    "\n",
    "aggregated_top_predictors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1-month yield</th>\n",
       "      <th>2-month yield</th>\n",
       "      <th>3-month yield</th>\n",
       "      <th>6-month yield</th>\n",
       "      <th>1-year yield</th>\n",
       "      <th>2-year yield</th>\n",
       "      <th>3-year yield</th>\n",
       "      <th>5-year yield</th>\n",
       "      <th>7-year yield</th>\n",
       "      <th>10-year yield</th>\n",
       "      <th>Long Term yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(123263942, 0.10557984041655372)</td>\n",
       "      <td>(123263942, 0.10797517659795748)</td>\n",
       "      <td>(Oil, 0.07383203962317605)</td>\n",
       "      <td>(39057, 0.07719988077315565)</td>\n",
       "      <td>(37810, 0.07796951440414075)</td>\n",
       "      <td>(37810, 0.11006354496857955)</td>\n",
       "      <td>(2710134, 0.0999196120862902)</td>\n",
       "      <td>(123263939, 0.0887023464782446)</td>\n",
       "      <td>(52299983, 0.08958903169822795)</td>\n",
       "      <td>(123263939, 0.09247256821517279)</td>\n",
       "      <td>(Oil, 0.11911608752291405)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Oil, 0.09845001840565236)</td>\n",
       "      <td>(Oil, 0.07044729425645786)</td>\n",
       "      <td>(39057, 0.061748188207936525)</td>\n",
       "      <td>(37810, 0.07390622549063965)</td>\n",
       "      <td>(39057, 0.06411721809610012)</td>\n",
       "      <td>(39057, 0.09593060053968856)</td>\n",
       "      <td>(123263939, 0.08006613256694704)</td>\n",
       "      <td>(52299983, 0.08287576030979174)</td>\n",
       "      <td>(Oil, 0.0875279665772665)</td>\n",
       "      <td>(123263908, 0.052471317552881626)</td>\n",
       "      <td>(123263939, 0.11480696227252458)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(123263939, 0.060637572786842474)</td>\n",
       "      <td>(123263939, 0.07015092521268927)</td>\n",
       "      <td>(123263942, 0.06169846045482405)</td>\n",
       "      <td>(Oil, 0.04571527401337534)</td>\n",
       "      <td>(123263939, 0.05462618403393764)</td>\n",
       "      <td>(123263939, 0.09037884854202512)</td>\n",
       "      <td>(122537, 0.07927347053408133)</td>\n",
       "      <td>(123263908, 0.06642497439835554)</td>\n",
       "      <td>(123263939, 0.08647180033742309)</td>\n",
       "      <td>(Oil, 0.050098481605630706)</td>\n",
       "      <td>(123263942, 0.04864668248245352)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(39052, 0.04794689222356353)</td>\n",
       "      <td>(39056, 0.06367560517559293)</td>\n",
       "      <td>(122536, 0.049678958613950226)</td>\n",
       "      <td>(123263939, 0.04282329862435774)</td>\n",
       "      <td>(39056, 0.048247781568158475)</td>\n",
       "      <td>(2710140, 0.04593957807841473)</td>\n",
       "      <td>(123263908, 0.06114833335367815)</td>\n",
       "      <td>(122537, 0.05486879955832039)</td>\n",
       "      <td>(123263908, 0.0757827129428825)</td>\n",
       "      <td>(2710134, 0.04780232606239468)</td>\n",
       "      <td>(2710140, 0.0446927766380585)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(39056, 0.04490152441513863)</td>\n",
       "      <td>(39053, 0.05109474136161087)</td>\n",
       "      <td>(39056, 0.04507991464268269)</td>\n",
       "      <td>(39056, 0.042512490396399205)</td>\n",
       "      <td>(122534, 0.04459356269070525)</td>\n",
       "      <td>(122535, 0.045436234683772045)</td>\n",
       "      <td>(122144, 0.05162178512639409)</td>\n",
       "      <td>(Oil, 0.05219603198568758)</td>\n",
       "      <td>(31185443, 0.0462168712391784)</td>\n",
       "      <td>(52299983, 0.04413911987406274)</td>\n",
       "      <td>(39067, 0.03862212278238173)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(39051, 0.04287770727470947)</td>\n",
       "      <td>(39055, 0.04534678237730316)</td>\n",
       "      <td>(122535, 0.044443851025833286)</td>\n",
       "      <td>(122535, 0.040152481233561486)</td>\n",
       "      <td>(122536, 0.044205690737710364)</td>\n",
       "      <td>(Oil, 0.04339247605401508)</td>\n",
       "      <td>(2710140, 0.042543197094528154)</td>\n",
       "      <td>(31185443, 0.04853089546829619)</td>\n",
       "      <td>(2710140, 0.042468930563222144)</td>\n",
       "      <td>(123263942, 0.043917801471505466)</td>\n",
       "      <td>(120573154, 0.03285609762863543)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(37833, 0.041856263672106334)</td>\n",
       "      <td>(39051, 0.04404144020560958)</td>\n",
       "      <td>(39052, 0.04282171798263738)</td>\n",
       "      <td>(3469253, 0.03996297934895922)</td>\n",
       "      <td>(122535, 0.04375756200605004)</td>\n",
       "      <td>(122536, 0.04069992330274575)</td>\n",
       "      <td>(3469253, 0.04195884976030512)</td>\n",
       "      <td>(2710140, 0.04612400678094665)</td>\n",
       "      <td>(80691335, 0.03671330247110859)</td>\n",
       "      <td>(2710140, 0.04185200601361183)</td>\n",
       "      <td>(122537, 0.030558855428977127)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(39053, 0.03695756343198507)</td>\n",
       "      <td>(39052, 0.043942822660720186)</td>\n",
       "      <td>(122534, 0.03787011380435713)</td>\n",
       "      <td>(122534, 0.0376506909567919)</td>\n",
       "      <td>(39052, 0.03724262046270019)</td>\n",
       "      <td>(39056, 0.03360207414293776)</td>\n",
       "      <td>(Oil, 0.037446436548623445)</td>\n",
       "      <td>(80691335, 0.04369498840326395)</td>\n",
       "      <td>(122537, 0.033301240269151926)</td>\n",
       "      <td>(31185443, 0.0378455819351175)</td>\n",
       "      <td>(3469253, 0.02508198647019755)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(39055, 0.03516447467527049)</td>\n",
       "      <td>(39054, 0.043597190784053254)</td>\n",
       "      <td>(39051, 0.03641010292309032)</td>\n",
       "      <td>(2710147, 0.03695384727399179)</td>\n",
       "      <td>(Oil, 0.035386832309886734)</td>\n",
       "      <td>(122534, 0.03329537586173452)</td>\n",
       "      <td>(39063, 0.02834844978669817)</td>\n",
       "      <td>(2710134, 0.040578159789499045)</td>\n",
       "      <td>(TSX, 0.027353688552073475)</td>\n",
       "      <td>(3469253, 0.030606681465638295)</td>\n",
       "      <td>(2710147, 0.024205601944439718)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(39067, 0.03137773518422805)</td>\n",
       "      <td>(2710147, 0.025957525179531607)</td>\n",
       "      <td>(39055, 0.03545887030018386)</td>\n",
       "      <td>(122536, 0.03675247166587878)</td>\n",
       "      <td>(39051, 0.034910053688562294)</td>\n",
       "      <td>(39052, 0.02325287387175046)</td>\n",
       "      <td>(80691335, 0.0255571225699981)</td>\n",
       "      <td>(TSX, 0.03977137628828866)</td>\n",
       "      <td>(123263942, 0.026174040428921824)</td>\n",
       "      <td>(80691335, 0.02414144968428014)</td>\n",
       "      <td>(122550, 0.024129654244155634)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       1-month yield                     2-month yield  \\\n",
       "0   (123263942, 0.10557984041655372)  (123263942, 0.10797517659795748)   \n",
       "1         (Oil, 0.09845001840565236)        (Oil, 0.07044729425645786)   \n",
       "2  (123263939, 0.060637572786842474)  (123263939, 0.07015092521268927)   \n",
       "3       (39052, 0.04794689222356353)      (39056, 0.06367560517559293)   \n",
       "4       (39056, 0.04490152441513863)      (39053, 0.05109474136161087)   \n",
       "5       (39051, 0.04287770727470947)      (39055, 0.04534678237730316)   \n",
       "6      (37833, 0.041856263672106334)      (39051, 0.04404144020560958)   \n",
       "7       (39053, 0.03695756343198507)     (39052, 0.043942822660720186)   \n",
       "8       (39055, 0.03516447467527049)     (39054, 0.043597190784053254)   \n",
       "9       (39067, 0.03137773518422805)   (2710147, 0.025957525179531607)   \n",
       "\n",
       "                      3-month yield                     6-month yield  \\\n",
       "0        (Oil, 0.07383203962317605)      (39057, 0.07719988077315565)   \n",
       "1     (39057, 0.061748188207936525)      (37810, 0.07390622549063965)   \n",
       "2  (123263942, 0.06169846045482405)        (Oil, 0.04571527401337534)   \n",
       "3    (122536, 0.049678958613950226)  (123263939, 0.04282329862435774)   \n",
       "4      (39056, 0.04507991464268269)     (39056, 0.042512490396399205)   \n",
       "5    (122535, 0.044443851025833286)    (122535, 0.040152481233561486)   \n",
       "6      (39052, 0.04282171798263738)    (3469253, 0.03996297934895922)   \n",
       "7     (122534, 0.03787011380435713)      (122534, 0.0376506909567919)   \n",
       "8      (39051, 0.03641010292309032)    (2710147, 0.03695384727399179)   \n",
       "9      (39055, 0.03545887030018386)     (122536, 0.03675247166587878)   \n",
       "\n",
       "                       1-year yield                      2-year yield  \\\n",
       "0      (37810, 0.07796951440414075)      (37810, 0.11006354496857955)   \n",
       "1      (39057, 0.06411721809610012)      (39057, 0.09593060053968856)   \n",
       "2  (123263939, 0.05462618403393764)  (123263939, 0.09037884854202512)   \n",
       "3     (39056, 0.048247781568158475)    (2710140, 0.04593957807841473)   \n",
       "4     (122534, 0.04459356269070525)    (122535, 0.045436234683772045)   \n",
       "5    (122536, 0.044205690737710364)        (Oil, 0.04339247605401508)   \n",
       "6     (122535, 0.04375756200605004)     (122536, 0.04069992330274575)   \n",
       "7      (39052, 0.03724262046270019)      (39056, 0.03360207414293776)   \n",
       "8       (Oil, 0.035386832309886734)     (122534, 0.03329537586173452)   \n",
       "9     (39051, 0.034910053688562294)      (39052, 0.02325287387175046)   \n",
       "\n",
       "                       3-year yield                      5-year yield  \\\n",
       "0     (2710134, 0.0999196120862902)   (123263939, 0.0887023464782446)   \n",
       "1  (123263939, 0.08006613256694704)   (52299983, 0.08287576030979174)   \n",
       "2     (122537, 0.07927347053408133)  (123263908, 0.06642497439835554)   \n",
       "3  (123263908, 0.06114833335367815)     (122537, 0.05486879955832039)   \n",
       "4     (122144, 0.05162178512639409)        (Oil, 0.05219603198568758)   \n",
       "5   (2710140, 0.042543197094528154)   (31185443, 0.04853089546829619)   \n",
       "6    (3469253, 0.04195884976030512)    (2710140, 0.04612400678094665)   \n",
       "7       (Oil, 0.037446436548623445)   (80691335, 0.04369498840326395)   \n",
       "8      (39063, 0.02834844978669817)   (2710134, 0.040578159789499045)   \n",
       "9    (80691335, 0.0255571225699981)        (TSX, 0.03977137628828866)   \n",
       "\n",
       "                        7-year yield                      10-year yield  \\\n",
       "0    (52299983, 0.08958903169822795)   (123263939, 0.09247256821517279)   \n",
       "1          (Oil, 0.0875279665772665)  (123263908, 0.052471317552881626)   \n",
       "2   (123263939, 0.08647180033742309)        (Oil, 0.050098481605630706)   \n",
       "3    (123263908, 0.0757827129428825)     (2710134, 0.04780232606239468)   \n",
       "4     (31185443, 0.0462168712391784)    (52299983, 0.04413911987406274)   \n",
       "5    (2710140, 0.042468930563222144)  (123263942, 0.043917801471505466)   \n",
       "6    (80691335, 0.03671330247110859)     (2710140, 0.04185200601361183)   \n",
       "7     (122537, 0.033301240269151926)     (31185443, 0.0378455819351175)   \n",
       "8        (TSX, 0.027353688552073475)    (3469253, 0.030606681465638295)   \n",
       "9  (123263942, 0.026174040428921824)    (80691335, 0.02414144968428014)   \n",
       "\n",
       "                    Long Term yield  \n",
       "0        (Oil, 0.11911608752291405)  \n",
       "1  (123263939, 0.11480696227252458)  \n",
       "2  (123263942, 0.04864668248245352)  \n",
       "3     (2710140, 0.0446927766380585)  \n",
       "4      (39067, 0.03862212278238173)  \n",
       "5  (120573154, 0.03285609762863543)  \n",
       "6    (122537, 0.030558855428977127)  \n",
       "7    (3469253, 0.02508198647019755)  \n",
       "8   (2710147, 0.024205601944439718)  \n",
       "9    (122550, 0.024129654244155634)  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_top_predictors2={}\n",
    "for k,v in aggregated_top_predictors.items():\n",
    "    aggregated_top_predictors2[k]=v[:10]\n",
    "\n",
    "top_predictos=pd.DataFrame(aggregated_top_predictors2)\n",
    "top_predictos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vectorid</th>\n",
       "      <th>cubeTitleEn</th>\n",
       "      <th>vector title</th>\n",
       "      <th>vector_id2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v36685</td>\n",
       "      <td>Chartered banks, assets and liabilities, month...</td>\n",
       "      <td>Total, major assets (Terminated) 4 5</td>\n",
       "      <td>36685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v36867</td>\n",
       "      <td>Chartered banks, assets and liabilities, month...</td>\n",
       "      <td>Personal loan plan loans</td>\n",
       "      <td>36867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v36868</td>\n",
       "      <td>Chartered banks, assets and liabilities, month...</td>\n",
       "      <td>Credit card loans</td>\n",
       "      <td>36868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v36869</td>\n",
       "      <td>Chartered banks, assets and liabilities, month...</td>\n",
       "      <td>Personal lines of credit</td>\n",
       "      <td>36869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v36870</td>\n",
       "      <td>Chartered banks, assets and liabilities, month...</td>\n",
       "      <td>Other personal loans</td>\n",
       "      <td>36870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>v111666248</td>\n",
       "      <td>Daily average foreign exchange rates in Canadi...</td>\n",
       "      <td>U.S. dollar, daily average</td>\n",
       "      <td>111666248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>v111666249</td>\n",
       "      <td>Daily average foreign exchange rates in Canadi...</td>\n",
       "      <td>Vietnamese dong, daily average (Terminated)</td>\n",
       "      <td>111666249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>v730416</td>\n",
       "      <td>Canada Mortgage and Housing Corporation, housi...</td>\n",
       "      <td>Housing starts</td>\n",
       "      <td>730416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>v52299983</td>\n",
       "      <td>Canada Mortgage and Housing Corporation, housi...</td>\n",
       "      <td>Total units</td>\n",
       "      <td>52299983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>v62295562</td>\n",
       "      <td>Gross domestic product (GDP) at basic prices, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62295562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>521 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       vectorid                                        cubeTitleEn  \\\n",
       "0        v36685  Chartered banks, assets and liabilities, month...   \n",
       "1        v36867  Chartered banks, assets and liabilities, month...   \n",
       "2        v36868  Chartered banks, assets and liabilities, month...   \n",
       "3        v36869  Chartered banks, assets and liabilities, month...   \n",
       "4        v36870  Chartered banks, assets and liabilities, month...   \n",
       "..          ...                                                ...   \n",
       "516  v111666248  Daily average foreign exchange rates in Canadi...   \n",
       "517  v111666249  Daily average foreign exchange rates in Canadi...   \n",
       "518     v730416  Canada Mortgage and Housing Corporation, housi...   \n",
       "519   v52299983  Canada Mortgage and Housing Corporation, housi...   \n",
       "520   v62295562  Gross domestic product (GDP) at basic prices, ...   \n",
       "\n",
       "                                    vector title vector_id2  \n",
       "0           Total, major assets (Terminated) 4 5      36685  \n",
       "1                       Personal loan plan loans      36867  \n",
       "2                              Credit card loans      36868  \n",
       "3                       Personal lines of credit      36869  \n",
       "4                           Other personal loans      36870  \n",
       "..                                           ...        ...  \n",
       "516                   U.S. dollar, daily average  111666248  \n",
       "517  Vietnamese dong, daily average (Terminated)  111666249  \n",
       "518                               Housing starts     730416  \n",
       "519                                  Total units   52299983  \n",
       "520                                          NaN   62295562  \n",
       "\n",
       "[521 rows x 4 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the selected vectors\n",
    "#os.chdir(\"/Users/alrz-/Library/CloudStorage/OneDrive-Personal/Backup/Yield Curve Pricing/Data\")\n",
    "csv_file_path = 'Vectors_selected.csv'\n",
    "Vectors_selected = pd.read_csv(csv_file_path, encoding='latin-1')\n",
    "Vectors_selected=Vectors_selected[[\"vectorid\", \"cubeTitleEn\" , \"vector title\"]]\n",
    "Vectors_selected.tail()\n",
    "Vectors_selected['vector_id2']=[x[1:] if isinstance(x, str) and x[0]==\"v\" else x for x in Vectors_selected[\"vectorid\"]]\n",
    "Vectors_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_top2=pd.DataFrame()\n",
    "\n",
    "for column in top_predictos.columns:\n",
    "    v=[]\n",
    "    impact=[]\n",
    "    for i in top_predictos[column]:\n",
    "        v.append(i[0])\n",
    "        impact.append(i[1])\n",
    "    \n",
    "    df_top=pd.DataFrame([v,impact]).transpose()\n",
    "    df_top.columns=[\"vector\",\"impact\"]\n",
    "    df_top=pd.merge(df_top,Vectors_selected,how='left' ,left_on=\"vector\" , right_on=\"vector_id2\")\n",
    "\n",
    "    df_top['vector title'] = df_top['vector title'].fillna(df_top['vector'])\n",
    "    df_top = df_top.drop(columns=['vector'])\n",
    "    df_top=df_top[['vector title','cubeTitleEn','impact']]\n",
    "    df_top[\"yield\"]=column\n",
    "    df_top2=pd.concat([df_top2,df_top],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top2.to_excel(\"top_predictors.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
